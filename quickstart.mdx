---
title: "Quickstart"
description: "All you need to get started"
icon: "rocket"
---

## Introduction

With **Galtea**, you can improve AI reliability, reduce risks, streamline compliance, and accelerates time to market registration. In this guide we explain all that you need to get started with basic evaluation and track the reliability of your LLM based products.

## Creating a product

The first step in tracking the quality and the reliability of your LLM based product is creating a product in the dashboard. For that you will need to login with your credentials.

![login to the platform](/images/login.png)

As you click on Products > Create New Product, you will be presented with the product onboarding form. The stored metadata is used mostly for repoorting purposes, and the description is used during the process of generating synthetic test data when needed.

![product onboarding](/images/product-onboarding.png)


## Install the SDK and connect via an API key

Once you create your product, now the rest of the process can be conveniently followed from the sdk. But in order to do that, you will need your API key, which can be acquired from Settings > Generate API key

![get api key](/images/api-key.png)

Once you have the API key, you will need to install the galtea library in your virtual environment

```
pip install galtea
```

Once installed, you can now check if the connection to the platform works correctly:

```python
from galtea import Galtea

galtea = Galtea(api_key=API_KEY)
products = galtea.products.list()
new_product_id = products[-1].id
print(products)
```

This should give you a list of products that you have created within your organization. For now you have only one product, and we got the id of the last created product, in the variable

## Registering a version

The main advantage of the Galtea platform is the ability to track and compare different versions of an LLM based product. A new version consists of new prompts, different RAG parameters, or an end-to-end product with new components. Before we launch an evaluation we would like to register a new version to the platform.

```
version = galtea.versions.create(
    name=f"test_version1",
    product_id=new_product_id,
    optional_props={
        "description": "a new version for the tutorial"
    }
)
```

Normally we would put all the relevant information in the metadata, from the system prompt to the other components such as guardrails, however for the tutorial we leave it empty for now.

## Create a test

In order to compare the reliability of the LLM based product, we not only need to be able to compare between different versions, but we need subject each version to the same tests for a consistent comparison. For that we need to create a test in the platform. Based on your application they can be question answer pairs with the gold standard answers. Below are some examples 

```
import pandas as pd

# Data
d = [
    {"input": "What is the Capital of France?",
     "actual_output": "Paris",
     "expected_output": "Paris"},
    {"input": "What is the capital of Spain?",
     "actual_output": "Barcelona",
     "expected_output": "Madrid"}
]

# Create DataFrame
df = pd.DataFrame(d)
df.to_csv('tutorial_tests.csv')
```

Now that you have our tests in a file, you can push them to the platform.

```
test = galtea.tests.create(
    name=f"example-test-tutorial",
    type="QUALITY",
    product_id=new_product_id,
    test_file_path="tutorial_tests.csv"
)
```

Uploading the file ensures that the launched tests are always the same hence ensuring that the metrics between the versions are comparable. Once a test file is uploaded, there is the possibility to downloaded from the platform via the sdk:

```
test_file_path = galtea.tests.download(test, output_directory="./")
print(test_file_path)
```

### Create a metric 

Galtea platform comes out of the box with pre-defined metrics which can be accessed via sdk

```python
metrics = galtea.metrics.list()
print(metrics)
```

However you will want to define your own metrics for your specific use-case, which is what we will do for the tutorial.

```
metric_self_accuracy = galtea.metrics.create(name="accuracy_v0", criteria="Determine whether the actual output is equivalent to the expected output.")
```

### Launch an evaluation

Finally we are ready to launch an evaluation. We start by creating an evaluation.

```
evaluation = galtea.evaluations.create(test_id=test.id, version_id=version.id)
```

Once the evaluation is created we can send the evaluation tasks one by one via the sdk

```
galtea.evaluate(
  metrics=[metric_accuracy.name],
  evaluation_id=evaluation.id,
  input="What is the capital of Spain?",
  actual_output="Barcelona", 
  expected_output="Madrid"
)
```

However it is recommended to use the test we have created which now already in the platform. Using the data frame.