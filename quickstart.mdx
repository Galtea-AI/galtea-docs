---
title: "Quickstart"
description: "All you need to get started with Galtea evaluations"
icon: "rocket"
---

import ProductCard from '/snippets/cards/product-card.mdx';
import VersionCard from '/snippets/cards/version-card.mdx';
import TestCard from '/snippets/cards/test-card.mdx';
import SessionCard from '/snippets/cards/session-card.mdx';
import InferenceResultCard from '/snippets/cards/inference-result-card.mdx';
import EvaluationCard from '/snippets/cards/evaluation-card.mdx';
import EvaluationTaskCard from '/snippets/cards/evaluation-task-card.mdx';
import MetricTypeCard from '/snippets/cards/metric-type-card.mdx';
import ModelCard from '/snippets/cards/model-card.mdx';

This guide will walk you through the steps to begin evaluating and monitoring the reliability of your AI products with Galtea as quickly as possible.

## Evaluation Workflow Overview

<Steps>
  <Step title="Create a Product">Define what functionality or service you want to evaluate on the Galtea platform.</Step>
  <Step title="Install SDK & Connect">Set up the Galtea Python SDK to interact with the platform.</Step>
  <Step title="Register a Version">Document a specific implementation of your product using the SDK.</Step>
  <Step title="Select a Test">Use a default Galtea test (or create your own) to challenge your product.</Step>
  <Step title="Select a Metric">Use a default Galtea metric (or define your own) to evaluate your product's version.</Step>
  <Step title="Run Evaluations">Test your product version with the selected test and metrics, then analyze results.</Step>
</Steps>

## 1. Create a Product

Create a [product](/concepts/product) in the Galtea dashboard. Navigate to **[Products](https://platform.galtea.ai/) > Create New Product** and complete the form.

<Info>
  The product description is important as it may be used to generate synthetic test data.
</Info>

## 2. Install the SDK and Connect

<Steps>
  <Step title="Get your API key">
    In the [Galtea dashboard](https://platform.galtea.ai/), navigate to **[Settings](https://platform.galtea.ai/settings) > Generate API Key** and copy your key.
  </Step>
  <Step title="Install the SDK">
    ```sh
    pip install galtea
    ```
  </Step>
  <Step title="Connect to the platform">
    ```python
    from galtea import Galtea
    
    galtea = Galtea(api_key="YOUR_API_KEY")
    
    products = galtea.products.list() 
    print(f"Found {len(products)} products.")
    # Choose a product ID for the next steps
    YOUR_PRODUCT_ID = products[0].id if products else "your_product_id"
    ```
  </Step>
</Steps>

## 3. Create a Version

Create a [version](/concepts/product/version) to track a specific implementation of your product.

```python
version = galtea.versions.create(
    name="v0.1-quickstart",
    product_id=YOUR_PRODUCT_ID,
    description="Initial version for quickstart evaluation"
)
print(f"Created Version with ID: {version.id}")
```

## 4. Use a Default Test

For this quickstart, we'll use the default "Jailbreak" [test](/concepts/product/test), which is a type of [Red Teaming Test](/concepts/product/test/red-teaming-tests).

```python
test = galtea.tests.get_by_name(product_id=YOUR_PRODUCT_ID, test_name="Jailbreak")
test_cases = galtea.test_cases.list(test_id=test.id)
print(f"Using test '{test.name}' with {len(test_cases)} test cases.")
```

## 5. Use a Default Metric

To evaluate the "Jailbreak" test, we'll use the "Jailbreak Resilience" [metric](/concepts/metric-type).

```python
metric = galtea.metrics.get_by_name(name="Jailbreak Resilience")
```

## 6. Run Evaluations

Now, run an [evaluation](/concepts/product/evaluation) by creating [evaluation tasks](/concepts/product/evaluation/task).

<Note>
  In a real scenario, `your_product_function` would be a call to your actual AI model.
</Note>

```python
# Placeholder for your actual product/model inference function
def your_product_function(input_prompt):
    if "ignore" in input_prompt.lower():
        return "I am programmed to follow safety guidelines and cannot fulfill this request."
    return f"Of course! I will now {input_prompt}"

# An evaluation is created implicitly with the first task.
# Loop through test cases and create evaluation tasks.
for test_case in test_cases:
    actual_output = your_product_function(test_case.input)
    
    galtea.evaluation_tasks.create_single_turn(
        version_id=version.id,
        test_case_id=test_case.id,
        metrics=[metric.name],
        actual_output=actual_output
    )

print(f"Submitted evaluation tasks for version {version.id} using test '{test.name}'.")
```

## 7. View Results

You can view results on the [Galtea dashboard](https://platform.galtea.ai/). Navigate to your product's "Analytics" tab to see detailed analysis and compare versions.

```python
print(f"View results at: https://platform.galtea.ai/product/{YOUR_PRODUCT_ID}?tab=1")
```

## Next Steps

Congratulations! You've completed your first evaluation with Galtea using default assets. This is just the beginning. Explore these concepts to tailor Galtea to your specific needs:

<CardGroup cols={3}>
  <ProductCard />
  <VersionCard />
  <TestCard />
  <SessionCard />
  <InferenceResultCard />
  <EvaluationCard />
  <EvaluationTaskCard />
  <MetricTypeCard />
  <ModelCard />
</CardGroup>

If you have any questions or need assistance, contact us at [support@galtea.ai](mailto:support@galtea.ai).