---
title: 'Evaluating Conversations'
description: "Learn how to evaluate multi-turn conversations using Galtea's session-based workflow."
icon: "comments"
---

import SessionCard from '/snippets/cards/session-card.mdx';
import InferenceResultCard from '/snippets/cards/inference-result-card.mdx';
import EvaluationCard from '/snippets/cards/evaluation-card.mdx';
import EvaluationTaskCard from '/snippets/cards/evaluation-task-card.mdx';

To accurately evaluate interactions within a dialogue, you can use Galtea's session-based workflow. This approach allows you to log an entire conversation and then run evaluations on all of its turns at once.

Certain metrics are specifically designed for conversational analysis and require the full context:
  - **Role Adherence**: Measures how well the AI stays within its defined role.
  - **Knowledge Retention**: Assesses the model's ability to remember and use information from previous turns.
  - **Conversation Completeness**: Evaluates whether the conversation has reached a natural and informative conclusion.
  - **Conversation Relevancy**: Assesses whether each turn in the conversation is relevant to the ongoing topic.

### The Session-Based Workflow

<Steps>
    <Step title="Create a Session">
        A [Session](/concepts/product/version/session) acts as a container for all the turns in a single conversation. You create one at the beginning of an interaction.
    </Step>
    <Step title="Log Inference Results">
        Each user input and model output pair is an [Inference Result](/concepts/product/version/session/inference-result). You can log these turns individually or in a single batch call after the conversation ends. Using a batch call is more efficient.
    </Step>
    <Step title="Evaluate the Session">
        Once the session is logged, you can create evaluation tasks for the entire conversation using the `evaluation_tasks.create()` method. This will generate tasks for each turn against the specified metrics.
    </Step>
</Steps>

### Example

This example demonstrates logging and evaluating a multi-turn conversation from a test case.

```python
from galtea import Galtea
import os

galtea = Galtea(api_key=os.getenv("GALTEA_API_KEY"))

YOUR_VERSION_ID = "your_version_id"
YOUR_TEST_CASE_ID = "your_test_case_id"
CONVERSATIONAL_METRICS = [
    "role-adherence",
    "knowledge-retention",
]

# 1. Create a Session linked to a test case
session = galtea.sessions.create(
    version_id=YOUR_VERSION_ID,
    test_case_id=YOUR_TEST_CASE_ID,
)
print(f"Created Session: {session.id}")

# 2. Log the conversation turns.
# In a real scenario, you would dynamically collect these from your product's interaction.
conversation_turns = [
    {'role': 'user', 'content': 'Hello, what can you do?'},
    {'role': 'assistant', 'content': 'I can help you with all your queries related to our services.', 'retrieval_context': None},
    {'role': 'user', 'content': "What's your return policy?"},
    {'role': 'assistant', 'content': "Our return policy allows returns within 30 days.", 'retrieval_context': "Returns are accepted within 30 days of purchase."},
    {'role': 'user', 'content': "What if I lost the receipt?"},
    {'role': 'assistant', 'content': "A proof of purchase is required for all returns.", 'retrieval_context': "Proof of purchase is required for all returns."},
]

# Use create_batch for efficiency
galtea.inference_results.create_batch(
    session_id=session.id,
    conversation_turns=conversation_turns
)

# 3. Evaluate the entire session at once
evaluation_tasks = galtea.evaluation_tasks.create(
    session_id=session.id,
    metrics=CONVERSATIONAL_METRICS
)
print(f"Submitted {len(evaluation_tasks)} evaluation tasks for session {session.id}")
```
<Note>
  This workflow can also be used for production monitoring by creating a session with `is_production=True` and omitting the `test_case_id`. See the [Monitor Production Responses](/sdk/tutorials/monitor-production-responses-to-user-queries) guide for an example.
</Note>

## Learn More

<CardGroup cols={2}>
  <SessionCard />
  <InferenceResultCard />
  <EvaluationCard />
  <EvaluationTaskCard />
</CardGroup>
