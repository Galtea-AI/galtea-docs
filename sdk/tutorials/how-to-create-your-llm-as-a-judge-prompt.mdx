---
title: 'How to create your LLM-as-a-judge prompt?'
description: "A comprehensive framework for constructing LLM-as-a-judge prompts through the specification of objective evaluation criteria and the definition of unambiguous scoring rubrics with precise thresholds."
icon: "pen"
---

You can use Galtea to [Evaluate with Custom Metrics](/sdk/tutorials/create-evaluation-with-custom-scores) and tailor your evaluation with a high degree of flexibility. However, this raises an important question: how do you translate the idea you want to evaluate into an effective, usable prompt?

## Key Components of a Judge Prompt

### 1. **Role (Optional)**

Clearly state what the evaluator is assessing:

```
**Role:** You are evaluating the _accuracy_ of the _ACTUAL_OUTPUT_.

### 2. **Evaluation Criteria (Essential)**


**What are Evaluation Criteria?**

Evaluation criteria are the clear rules or checkpoints you want the judge (LLM or human) to use when reviewing an output. They help make your evaluation process fair, repeatable, and easy to understand.

**Recommendations:**
- **Be specific:** Do not use vague terms like "good quality." Instead, explain exactly what quality means for your use case.
- **Break it down (itemization):** Breaking down your criteria into separate, clearly defined points makes them easier to check, score, and understand. Each criterion should focus on one thing at a time.
- **Use objective signals:** Look for things you can see or measure, not just opinions or feelings.
- **Give examples:** Show what a good answer looks like, and what a bad answer looks like, for each criterion.
- **Number your criteria:** This makes them easier to reference and ensures you don't miss anything important.

**Structure:**
```
For each [unit of analysis]:
1. Check that [specific condition A]
2. Verify that [specific condition B]
3. Ensure that [specific condition C]
```

### 3. **Scoring Rubric (Essential)**

The scoring rubric is how you decide what score to give for each output. It sets the rules for what counts as a pass or fail, and helps make your evaluation fair and consistent. Here are the main things to keep in mind:

**A. Define clear thresholds.** Binary (0/1): Specify exactly what counts as a pass or a fail. Make your scoring rules easy to understand and apply.

**B. Use consistent logic:**
- **All-or-nothing (recommended for direct validation of your system):** Decide whether all criteria must be met for a passing score, or if a single failure results in a fail.
- **Counting-based:** You can also score based on how many criteria are satisfied.

**C. Cover edge cases:** Think about what happens if criteria conflict, if data is missing, or if something is unclear. Make sure your rubric explains how to handle these situations.

**Structure for Binary:**
```
- Score 1: The ACTUAL_OUTPUT meets all the evaluation criteria you listed above. For example, it is accurate, complete, and follows all instructions.
- Score 0: The ACTUAL_OUTPUT fails one or more of the evaluation criteria. For example, it is missing key information, contains errors, or does not follow a specific instructions.
```

## Common Pitfalls to Avoid

1. **Vague criteria:** "Check if response is good" → Instead: "Check if response addresses all parts of the user's question"
2. **Ambiguous thresholds:** "Mostly correct" → Instead: "At least 3 out of 4 criteria met"
3. **Missing edge cases:** Not specifying what happens with partial matches or ambiguous data
4. **Subjective language:** "Natural" or "appropriate" without defining what that means
5. **Overlapping criteria:** Multiple criteria testing the same thing differently

## Template

```
**Role:** You are evaluating [aspect] of [parameter].

**Evaluation Criteria:**

For each [unit of analysis], verify:

1. **[Criterion name]:** [Specific, measurable condition with examples]
2. **[Criterion name]:** [Specific, measurable condition with examples]
3. **[Criterion name]:** [Specific, measurable condition with examples]

**Scoring Rubric:**

- **Score 1:** [Clear threshold for success, e.g., "All evaluation criteria are met."]
- **Score 0:** [Clear threshold for failure, e.g., "One or more evaluation criteria are not met."]
```

<Note>
  This template serves as a starting skeleton that you can adapt and modify based on your specific evaluation needs. Feel free to add more criteria, adjust the scoring scale, or restructure sections to better fit your use case.
</Note>

The key is: **If you can't program it as a rule-based system, your criteria aren't specific enough for an LLM judge either.**

## How to implement it with Galtea?

Here's an example of how to create a custom metric using the template above:

```python

from datetime import datetime
from galtea import Galtea
from dotenv import load_dotenv
import os

load_dotenv()

galtea = Galtea(api_key=os.getenv("API_KEY"))

# Create a new metric
metric = galtea.metrics.create(
    name="Generic Metric",
    test_type="QUALITY",  # or "RED_TEAMING", "SCENARIO"
    source="partial_prompt",
    judge_prompt="""
    **Role:** You are evaluating [what aspect] of [what system/output].

    **Evaluation Criteria:**

    For each [unit of analysis], verify:

    1. **[Criterion name]:** [Specific, measurable condition with examples]
    2. **[Criterion name]:** [Specific, measurable condition with examples]
    3. **[Criterion name]:** [Specific, measurable condition with examples]

    **Scoring Rubric:**

    - **Score 1:** [Clear threshold for success, e.g., "All evaluation criteria are met."]
    - **Score 0:** [Clear threshold for failure, e.g., "One or more evaluation criteria are not met."]
    """,
    evaluator_model_name="GPT-4o",
    evaluation_params=["input", "actual_output"],
    description="Genertic Metric",
    tags=["quality"],
)
``` 