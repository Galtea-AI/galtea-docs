---
title: 'Run Evaluation Tasks'
description: "Learn how to run evaluation tasks using the SDK's test-based workflow"
icon: "clipboard-check"
---

To evaluate a product version, you run individual [evaluation tasks](/concepts/product/evaluation/task) against a set of [test cases](/concepts/product/test/case).

## Running Evaluation Tasks

You can loop through your test cases and create evaluation tasks for each one. This allows you to assess performance across multiple metrics simultaneously.

```python
from galtea import Galtea
from datetime import datetime

# Initialize Galtea SDK
galtea = Galtea(api_key="YOUR_API_KEY")

# Assume you have a version_id and test_id
VERSION_ID = "YOUR_VERSION_ID"
TEST_ID = "YOUR_TEST_ID"

# Load your test cases
test_cases = galtea.test_cases.list(test_id=TEST_ID)

# Your product's inference function (placeholder)
def your_product_function(input_prompt, context=None):
    # In a real scenario, this would call your model
    return f"Model response to: {input_prompt}"

# Evaluate all test cases
for test_case in test_cases:
    # Your product's actual response to the input
    actual_output = your_product_function(test_case.input, test_case.context)
    
    # Run evaluation task
    galtea.evaluation_tasks.create_single_turn(
        version_id=VERSION_ID,
        test_case_id=test_case.id,
        metrics=["accuracy_v1", "relevance_v1"],
        actual_output=actual_output,
    )

print("All evaluation tasks submitted.")
```

<Note>
    The `metrics` parameter specifies which metric types to use.
</Note>
<Info>
    A [session](/concepts/product/session) and [evaluation](/concepts/product/evaluation) is automatically created behind the scenes to link this `version_id` and `test_id` with the provided [inference result](/concepts/product/inference-result) (the `actual_output` and the [Test Case](/concepts/product/test/case)'s input).
</Info>
