---
title: 'Monitor production responses'
description: "Learn how to create evaluation tasks from user queries in production for deeper analysis"
icon: "users"
# iconType: "solid"
---

[Evaluations](/concepts/product/evaluation) allow you to assess how well a specific [version](/concepts/product/version) of your [product](/concepts/product) performs against real user queries in production by running individual [evaluation tasks](/concepts/product/evaluation/task).

## Instantiate the SDK

To use the Galtea SDK, you need to initialize it with your API key. This is typically done at the beginning of your script:

  ```python
  from galtea import Galtea

  galtea = Galtea(api_key="YOUR_API_KEY")
  ```

## Capturing Production Data for Analysis

You can create evaluation tasks directly from real user queries in your production environment. All this data is stored for deeper analysis, allowing you to track performance over time:

```python
from datetime import datetime

# When your product handles a real user query in production
timeBeforeCall = datetime.now()
response = your_product_function(user_query, context, retrieval_context)
timeAfterCall = datetime.now()

# Create an evaluation task from this production interaction
evaluation_tasks = galtea.evaluation_tasks.create_from_production(
  version_id=VERSION_ID,
  metrics=["coherence", metric.name, "answer relevancy"],
  input=user_query,
  actual_output=response,
  retrieval_context=retrieval_context,
  context=context,
  conversation_turns=extract_conversation_turns(conversation_history),
  usage_info={
    "input_tokens": response.input_tokens,
    "output_tokens": response.output_tokens,
    "cache_read_input_tokens": response.cache_read_input_tokens,
  },
  latency=(timeAfterCall - timeBeforeCall).total_seconds() * 1000
)
```

<Info>
  By capturing evaluation data from production, you build a valuable repository of real-world performance metrics that can be analyzed later to identify trends, issues, and opportunities for improvement.
</Info>
<Note>
  The `metrics` parameter specifies which [metric types](/concepts/metric-type) to use for evaluating the task. You can use multiple metrics simultaneously to get different perspectives on performance.
  
  The `latency` and `usage_info` parameters are optional but highly recommended to keep track of the the real performance data of you product. 
</Note>

## Evaluating conversations

To accurately **evaluate interactions within a dialogue**, certain metrics need access to the preceding turns of the conversation. This is handled by the [ `conversation_turns` parameter](/sdk/api/evaluation-task/service#param-conversation-turns-1).

Currently, this parameter can only be used with the following metrics, which are available by default:
  - **Role Adherence**: Measures how well the actual output adheres to a specified role.
  - **Knowledge Retention**: Assesses the model's ability to retain and use information from previous turns in the conversation.
  - **Conversation Completeness**: Evaluates whether the conversation has reached a natural and informative conclusion.
  - **Conversation Relevancy**: Assesses whether each turn in the conversation is relevant to the ongoing topic and user needs.


### Expected format

The `conversation_turns` parameter expects a list of dictionaries. Each dictionary represents a **single, complete exchange** (one user query and the assistant's response to it) that occurred before the current interaction being sent to our platform.

The required structure for each dictionary in the list is:
```json
{
  "input": "The user's query from that past turn",
  "actual_output": "The assistant's response from that past turn"
}
```

This is how it would look like in practice:
```python
# Example: conversation_turns list passed to create_from_production
[
  {"input": "Hi, tell me about Galtea.", "actual_output": "Galtea is an AI evaluation platform."},
  {"input": "What metrics does it support?", "actual_output": "It supports accuracy, relevance, safety metrics, and more."},
  # ... other past turns in chronological order ...
]
```

### Converting Your Application's History Format

Let's say you have a conversation history like this.

```python
conversation_history = [
  {'system': 'system', 'content': 'You are a pirate.'},
  {'role': 'user', 'content': 'Can you teach me to swim?'},
  {'role': 'assistant', 'content': 'Sure thing, matey! Then ye go swim with the shark.'},
  {'role': 'user', 'content': 'I am not sure if I can swim with the shark.'},
  {'role': 'assistant', 'content': 'Ye can do it!'},
]
```
Then, this would be a valid method to extract the conversation turns from the conversation history:
```python
def extract_conversation_turns(messages: list[dict]) -> list[dict]:
  conversation_turns = []
  user_query = None
  for message in messages:
    # Skip system messages or irrelevant roles if necessary
    if message['role'] == 'system':
        continue

    if message['role'] == 'user':
        # Store the user query temporarily
        user_query = message['content']
    elif message['role'] == 'assistant' and user_query is not None:
        # Found an assistant response following a user query, form a turn
        conversation_turns.append({
            "input": user_query,
            "actual_output": message['content']
        })
        # Reset user_query to wait for the next user message
        user_query = None
    # Handle edge cases: consecutive user/assistant messages if applicable

  return conversation_turns
```