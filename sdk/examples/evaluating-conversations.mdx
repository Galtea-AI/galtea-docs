---
title: 'Evaluating Conversations'
description: "Learn how to evaluate multi-turn conversations using Galtea's session-based workflow."
icon: "comments"
---

import SessionCard from '/snippets/cards/session-card.mdx';
import InferenceResultCard from '/snippets/cards/inference-result-card.mdx';
import EvaluationCard from '/snippets/cards/evaluation-card.mdx';
import EvaluationTaskCard from '/snippets/cards/evaluation-task-card.mdx';

To accurately evaluate interactions within a dialogue, you can use Galtea's session-based workflow. This approach allows you to log an entire conversation and then run evaluations on all of its turns at once.

Certain metrics are specifically designed for conversational analysis and require the full context:
  - **Role Adherence**: Measures how well the AI stays within its defined role.
  - **Knowledge Retention**: Assesses the model's ability to remember and use information from previous turns.
  - **Conversational Completeness**: Evaluates whether the conversation has reached a natural and informative conclusion.
  - **Conversation Relevancy**: Assesses whether each turn in the conversation is relevant to the ongoing topic.

### The Session-Based Workflow

<Steps>
    <Step title="Create a Session">
        A [Session](/concepts/product/session) acts as a container for all the turns in a single conversation. You create one at the beginning of an interaction.
    </Step>
    <Step title="Log Inference Results">
        Each user input and model output pair is an [Inference Result](//concepts/product/inference-result). You can log these turns individually or in a single batch call after the conversation ends. Using a batch call is more efficient.
    </Step>
    <Step title="Evaluate the Session">
        Once the session is logged, you can create evaluation tasks for the entire conversation using the `evaluation_tasks.create()` method. This will generate tasks for each turn against the specified metrics.
    </Step>
</Steps>

### Example

Here is a complete example of logging and evaluating a multi-turn conversation.

```python
from galtea import Galtea

galtea = Galtea(api_key="YOUR_API_KEY")

YOUR_PRODUCT_ID = "your_product_id"
YOUR_VERSION_ID = "your_version_id"
YOUR_TEST_CASE_ID = "your_test_case_id"
YOUR_METRIC_NAME = "your_metric_name"

# 1. Create a Session to group a conversation
session = galtea.sessions.create(
    version_id=YOUR_VERSION_ID,
    test_case_id=YOUR_TEST_CASE_ID,
)
print(f"Created Session: {session.id}")

# 2. Suppose you have already the conversation with multiple turns.
conversation_turns = [
    {
        "input": "What's your return policy?",
        "output": "Our return policy allows returns within 30 days of purchase."
    },
    {
        "input": "What if I lost the receipt?",
        "output": "A proof of purchase is required for all returns."
    },
    {
        "input": "Can I return items bought online to a physical store?",
        "output": "Yes, you can return online purchases to any of our physical store locations."
    },
    {
        "input": "What about exchanges?",
        "output": "Exchanges follow the same 30-day policy and can be done in-store or online."
    }
]

# Use create_batch for efficient bulk processing
galtea.inference_results.create_batch(
    session_id=session.id,
    conversation_turns=conversation_turns
)

# Or loop through conversation turns. 
    # for turn in conversation_turns:
    #
    #     Creating Inference Results one by one is necessary if you want to log them as they happen.
    #     galtea.inference_results.create(
    #         session_id=session.id,
    #         input=turn["input"],
    #         output=turn["output"]
    #     )

print(f"Stored {len(conversation_turns)} inference results in session {session.id} with a single call")

# 3. When ready, create evaluation tasks for the entire session
evaluation_tasks = galtea.evaluation_tasks.create(
    session_id=session.id,
    metrics=[YOUR_METRIC_NAME]
)
print(f"Submitted {len(evaluation_tasks)} evaluation tasks for session {session.id}")
```

<Note>
For better performance with multiple conversation turns, it is highly recommended to use `create_batch()` instead of calling `create()` for each inference result in a loop.
</Note>

## Learn More

<CardGroup cols={2}>
  <SessionCard />
  <InferenceResultCard />
  <EvaluationCard />
  <EvaluationTaskCard />
</CardGroup>
