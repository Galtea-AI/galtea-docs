---
title: 'Evaluating Conversations'
description: "Learn how to evaluate multi-turn conversations using Galtea's session-based workflow."
icon: "comments"
---

To accurately evaluate interactions within a dialogue, you can use Galtea's session-based workflow. This approach allows you to log an entire conversation and then run evaluations on all of its turns at once.

Certain metrics are specifically designed for conversational analysis and require the full context:
  - **Role Adherence**: Measures how well the AI stays within its defined role.
  - **Knowledge Retention**: Assesses the model's ability to remember and use information from previous turns.
  - **Conversational Completeness**: Evaluates whether the conversation has reached a natural and informative conclusion.
  - **Conversation Relevancy**: Assesses whether each turn in the conversation is relevant to the ongoing topic.

### The Session-Based Workflow

<Steps>
    <Step title="Create a Session">
        A [Session](/concepts/product/session) acts as a container for all the turns in a single conversation. You create one at the beginning of an interaction.
    </Step>
    <Step title="Log Inference Results">
        Each user input and model output pair is an [Inference Result](//concepts/product/inference-result). You can log these turns individually or in a single batch call after the conversation ends. Using a batch call is more efficient.
    </Step>
    <Step title="Evaluate the Session">
        Once the session is logged, you can create evaluation tasks for the entire conversation using the `evaluation_tasks.create()` method. This will generate tasks for each turn against the specified metrics.
    </Step>
</Steps>

### Example

Here is a complete example of logging and evaluating a multi-turn conversation.

```python
from galtea import Galtea

# Initialize Galtea SDK
galtea = Galtea(api_key="YOUR_API_KEY")

YOUR_VERSION_ID = "your_version_id"

# 1. Create a Session for the conversation
# For production monitoring, set is_production=True and omit test_case_id
session = galtea.sessions.create(
    version_id=YOUR_VERSION_ID,
    is_production=True
)
print(f"Created Session: {session.id}")

# 2. Log all inference results in a single efficient batch call
conversation_turns = [
    {
        "input": "What's your return policy?",
        "output": "Our return policy allows returns within 30 days of purchase."
    },
    {
        "input": "What if I lost the receipt?",
        "output": "A proof of purchase is required for all returns."
    },
    {
        "input": "Can I return items bought online to a physical store?",
        "output": "Yes, you can return online purchases to any of our physical store locations."
    },
    {
        "input": "What about exchanges?",
        "output": "Exchanges follow the same 30-day policy and can be done in-store or online."
    }
]

# Use create_batch for efficient bulk processing
galtea.inference_results.create_batch(
    session_id=session.id,
    conversation_turns=conversation_turns
)
print(f"Stored {len(conversation_turns)} inference results in session {session.id}.")

# 3. When ready, create evaluation tasks for the entire session
evaluation_tasks = galtea.evaluation_tasks.create(
    session_id=session.id,
    metrics=["knowledge-retention", "conversational-relevancy"]
)
print(f"Submitted {len(evaluation_tasks)} evaluation tasks for session {session.id}.")

# You can now view the results in the Galtea dashboard.
```
<Note>
For better performance with multiple conversation turns, it is highly recommended to use `create_batch()` instead of calling `create()` for each inference result in a loop.
</Note>