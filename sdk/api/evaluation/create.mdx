---
title: 'Create Evaluations'
description: "Create evaluations for all inference results within a session using specified metrics."
---

This method evaluates an entire conversation stored in a [Session](/concepts/product/version/session) by creating evaluations for each of its [Inference Results](/concepts/product/version/session/inference-result).

## Returns
Returns a list of [Evaluation](/concepts/product/version/session/evaluation) objects, one for each metric and each inference result in the session.


## Example
```python
from galtea import Galtea

galtea = Galtea(api_key="YOUR_API_KEY")

# First, create a session and log inference results
session = galtea.sessions.create(version_id="YOUR_VERSION_ID")
galtea.inference_results.create_batch(
    session_id=session.id,
    conversation_turns=[
        {"role": "user", "content": "Hi"},
        {"role": "assistant", "content": "Hello!"},
        {"role": "user", "content": "How are you?"},
        {"role": "assistant", "content": "I am fine, thank you."}
    ]
)

# Now, evaluate the entire session using MetricInput format (recommended)
evaluations = galtea.evaluations.create(
    session_id=session.id,
    metrics=[
        {"name": "Role Adherence"}, 
        {"name": "Conversation Relevancy"}
    ]
)

# Alternative: String format (legacy, still supported)
evaluations = galtea.evaluations.create(
    session_id=session.id,
    metrics=["Role Adherence", "Conversation Relevancy"]
)
```

## Parameters
<ResponseField name="session_id" type="string" required>
  The ID of the session containing the inference results to be evaluated.
</ResponseField>
<ResponseField name="metrics" type="List[Union[str, Dict]]" required>
  A list of metrics to use for the evaluation. 
  
  **Recommended:** `MetricInput` dictionary format:
  ```python
  metrics=[
      {"name": "Role Adherence"},              # By name
      {"id": "metric_xyz"},                    # By ID
      {"name": "custom", "score": 0.95}        # With pre-computed score (for self-hosted metrics)
  ]
  ```
  
  **Also supported (legacy):**
  - **By name** (string): `metrics=["Role Adherence"]`
  
  The `MetricInput` dictionary supports the following keys:
  - `id` (string, optional): The ID of an existing metric
  - `name` (string, optional): The name of the metric (either `id` or `name` must be provided)
  - `score` (float, optional): A pre-computed score (0.0 to 1.0) for self-hosted metrics

</ResponseField>
