---
title: 'Create Inference Result'
description: "Create a new inference result log in a session."
---

## Returns
Returns an [InferenceResult](/concepts/product/version/session/inference-result) object.

## Example
```python
inference_result = galtea.inference_results.create(
    session_id="YOUR_SESSION_ID",
    input="What is the capital of France?",
    output="Paris is the capital of France."
)
```

## Parameters
<ResponseField name="session_id" type="string" required>
  The session ID to log the inference result to.
</ResponseField>
<ResponseField name="input" type="string" required>
  The input text/prompt.
</ResponseField>
<ResponseField name="output" type="string" required>
  The generated output/response.
</ResponseField>
<ResponseField name="retrieval_context" type="string" optional>
  Context retrieved for RAG systems.
</ResponseField>
<ResponseField name="latency" type="float" optional>
  Latency in milliseconds.
</ResponseField>
<ResponseField name="usage_info" type="dict[str, int]" optional>
  Information about token usage during the model call.
  Possible keys include:
  - `input_tokens`: Number of input tokens sent to the model.
  - `output_tokens`: Number of output tokens generated by the model.
  - `cache_read_input_tokens`: Number of input tokens read from the cache.
</ResponseField>
<ResponseField name="cost_info" type="dict[str, float]" optional>
  Information about the cost per token during the model call.
  Possible keys include:
  - `cost_per_input_token`: Cost per input token sent to the model.
  - `cost_per_output_token`: Cost per output token generated by the model.
  - `cost_per_cache_read_input_token`: Cost per input token read from the cache.
</ResponseField>

<ResponseField name="conversation_simulator_version" type="string" optional>
  The version of Galtea's conversation simulator used to generate the user message (input). This should only be provided when logging a conversation that was generated using the simulator.
</ResponseField>
