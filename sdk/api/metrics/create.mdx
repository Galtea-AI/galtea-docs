---
title: "Create Metric"
description: "Create a metric type for evaluating your products."
---

## Returns

Returns a [MetricType](/concepts/metric-type) object for the given parameters, or `None` if an error occurs.

## Examples

<Tabs>
  <Tab title="LLM-as-a-Judge">
    ```python
    metric = galtea.metrics.create(
        name="accuracy_v1",
        test_type="QUALITY",
        evaluator_model_name="GPT-4.1",
        judge_prompt="check if the {expected_output} is equal to the {actual_output}",
        tags=["custom", "accuracy"],
        description="A custom accuracy metric."
    )
    ```
  </Tab>
  <Tab title="Self Hosted">
    ```python
    metric = galtea.metrics.create(
        name="accuracy_v1",
        test_type="QUALITY",
        tags=["custom", "accuracy"],
        description="A custom accuracy metric calculated by our custom score function."
    )
    ```
  </Tab>
</Tabs>

## Parameters
<ResponseField name="name" type="string" required>
  The name of the metric.
</ResponseField>
<ResponseField name="test_type" type="string" required>
  The type of test this metric is designed for.
  Possible values: `QUALITY`, `RED_TEAMING`, `SCENARIOS`.
</ResponseField>
<ResponseField name="evaluator_model_name" type="string" optional>
  The name of the model used to evaluate the metric. Required for metrics using `judge_prompt`.
  
  **Available models:**
  - `"GPT-35-turbo"`
  - `"GPT-4o"`
  - `"GPT-4o-mini"`
  - `"GPT-4.1"`
  - `"Gemini-2.0-flash"`
  - `"Gemini-2.5-Flash"`
  - `"Gemini-2.5-Flash-Lite"`
  
  <Note>
    It should not be provided if the metric is "self hosted" (has no `judge_prompt`) since it does not require a model for evaluation.
  </Note>
</ResponseField>
<ResponseField name="judge_prompt" type="string" optional>
  A custom prompt that defines the evaluation logic for an LLM-as-a-judge metric. You can use placeholders like `{input}`, `{actual_output}`, etc., which will be populated at evaluation time. If you provide a `judge_prompt`, the metric will be an LLM-based evaluation. If omitted, the metric is considered a deterministic "Custom Score" metric.
</ResponseField>
<ResponseField name="tags" type="list[string]" optional>
  Tags to categorize the metric type.
</ResponseField>
<ResponseField name="description" type="string" optional>
  A brief description of what the metric evaluates.
</ResponseField>
<ResponseField name="documentation_url" type="string" optional>
  A URL pointing to more detailed documentation about the metric type.
</ResponseField>
