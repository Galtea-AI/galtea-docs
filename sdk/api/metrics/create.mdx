---
title: "Create Metric"
description: "Create a metric type for evaluating your products."
---

## Returns

Returns a [MetricType](/concepts/metric-type) object for the given parameters, or `None` if an error occurs.

## Examples

<Tabs>
  <Tab title="Geval">
    ```python
    metric = galtea.metrics.create(
        name="accuracy_v1",
        evaluator_model_name="GPT-4.1",
        criteria="Determine whether the actual output is equivalent to the expected output.",
        evaluation_params=["input", "expected_output", "actual_output"],
        tags=["custom", "accuracy"],
        description="A custom accuracy metric."
    )
    ```
  </Tab>
  <Tab title="Custom Judge">
    ```python
    metric = galtea.metrics.create(
        name="accuracy_v1",
        evaluator_model_name="GPT-4.1",
        judge_prompt="check if the {expected_output} is equal to the {actual_output}",
        tags=["custom", "accuracy"],
        description="A custom accuracy metric."
    )
    ```
  </Tab>
  <Tab title="Custom Score">
    ```python
    metric = galtea.metrics.create(
        name="accuracy_v1",
        tags=["custom", "accuracy"],
        description="A custom accuracy metric calculated by our custom score function."
    )
    ```
  </Tab>
</Tabs>

## Parameters
<ResponseField name="name" type="string" required>
  The name of the metric.
</ResponseField>
<ResponseField name="evaluator_model_name" type="string" optional>
  The name of the model used to evaluate the metric. This model will be used to assess the quality of the outputs based on the metric's criteria. 
  
  **Available models:**
  - `"GPT-35-turbo"`
  - `"GPT-4o"`
  - `"GPT-4o-mini"`
  - `"GPT-4.1"`
  - `"Gemini-2.0-flash"`
  - `"Gemini-2.5-Flash"`
  - `"Gemini-2.5-Flash-Lite"`
  
  <Note>
    It should not be provided if the metric is "custom" (has no *criteria* nor *evaluation_steps*) since it does not require a model for evaluation.
  </Note>
</ResponseField>
<ResponseField name="evaluation_params" type="list[string]" optional>
  A list of strings indicating which parameters are relevant for evaluating this metric.

  **Supported values for G-Eval metrics:**
  - `"input"`: The original prompt or query sent to the model (always required).
  - `"actual_output"`: The output generated by the model.
  - `"expected_output"`: The ideal or reference answer for the input.
  - `"context"`: Supplementary context or background information provided to the model.
  - `"retrieval_context"`: Information retrieved by your RAG pipeline to support inference.

  **Additional values for Custom Judge metrics:**
  - `"product_description"`: High-level description of the product being evaluated.
  - `"product_capabilities"`: Capabilities or intended functionalities of the product.
  - `"product_inabilities"`: Known limitations or things the product cannot or should not do.
  - `"product_security_boundaries"`: Specific boundaries or restrictions to ensure secure behavior.
</ResponseField>
<ResponseField name="criteria" type="string" required="conditional">
  The criteria for the metric.
</ResponseField>
<ResponseField name="evaluation_steps" type="list[string]" required="conditional">
  The evaluation steps for the metric.
  <Note>
    You need to provide **either** *Criteria* **or** *Evaluation Steps*, but not both. Your choice depends on your preferred evaluation approach.
  </Note>
</ResponseField>
<ResponseField name="judge_prompt" type="string" optional>
  The custom judge prompt for the metric.
</ResponseField>
<ResponseField name="tags" type="list[string]" optional>
  The tags for the metric type so it can be easily identified and categorized.
</ResponseField>
<ResponseField name="description" type="string" optional>
  A brief description of what the metric type evaluates.
</ResponseField>
<ResponseField name="documentation_url" type="string" optional>
  A URL pointing to more detailed documentation about the metric type.
</ResponseField>
