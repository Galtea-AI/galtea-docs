---
title: 'Create Metric'
description: "Create a metric type for evaluating your products."
---

## Returns
Returns a [MetricType](/concepts/metric-type) object for the given parameters, or `None` if an error occurs.

## Example
```python
metric = galtea.metrics.create(
    name="accuracy_v1",
    evaluator_model_name="GPT-4.1",
    criteria="Determine whether the actual output is equivalent to the expected output.",
    evaluation_params=["input", "expected output", "actual output"],
    tags=["custom", "accuracy"],
    description="A custom accuracy metric.",
    documentation_url="https://docs.example.com/accuracy_v1"
)
```

## Parameters
<ResponseField name="name" type="string" required>
  The name of the metric.
</ResponseField>
<ResponseField name="evaluator_model_name" type="string" optional>
  The name of the model used to evaluate the metric. This model will be used to assess the quality of the outputs based on the metric's criteria. 
  
  **Available models:**
  - `"GPT-4o"`
  - `"GPT-4.1"`
  - `"O3-mini"`
  - `"Gemini-2.0-flash"`
  - `"Gemini-2.5-Flash"`
  - `"Gemini-2.5-Flash-Lite"`
  
  <Note>
    It should not be provided if the metric is "custom" (has no *criteria* nor *evaluation_steps*) since it does not require a model for evaluation.
  </Note>
</ResponseField>
<ResponseField name="evaluation_params" type="list[string]" optional>
  A list of strings indicating which parameters are relevant for this metric's evaluation criteria or steps. If not provided, the metric will apply to tasks without specific parameter matching.
  Allowed values are:
  - `"input"` (The prompt or query sent to the model)
  - `"actual output"` (The actual output generated by the model)
  - `"expected output"` (The ideal answer for the given input)
  - `"context"` (Additional background information provided to the model)
  - `"retrieval context"` (The context retrieved by your RAG system)
</ResponseField>
<ResponseField name="criteria" type="string" required="conditional">
  The criteria for the metric.
</ResponseField>
<ResponseField name="evaluation_steps" type="list[string]" required="conditional">
  The evaluation steps for the metric.
  <Note>
    You need to provide **either** *Criteria* **or** *Evaluation Steps*, but not both. Your choice depends on your preferred evaluation approach.
  </Note>
</ResponseField>
<ResponseField name="tags" type="list[string]" optional>
  The tags for the metric type so it can be easily identified and categorized.
</ResponseField>
<ResponseField name="description" type="string" optional>
  A brief description of what the metric type evaluates.
</ResponseField>
<ResponseField name="documentation_url" type="string" optional>
  A URL pointing to more detailed documentation about the metric type.
</ResponseField>
