---
title: 'Create Evaluation Tasks'
description: "Create evaluation tasks for all inference results within a session using specified metrics."
---

This method evaluates an entire conversation stored in a [Session](/concepts/product/version/session) by creating evaluation tasks for each of its [Inference Results](/concepts/product/version/session/inference-result).

## Returns
Returns a list of [EvaluationTask](/concepts/product/version/session/evaluation/task) objects, one for each metric and each inference result in the session.


## Example
```python
from galtea import Galtea, EvaluationMetric

# First, create a session and log inference results
session = galtea.sessions.create(version_id="YOUR_VERSION_ID")
galtea.inference_results.create_batch(
    session_id=session.id,
    conversation_turns=[
        {"role": "user", "content": "Hi"},
        {"role": "assistant", "content": "Hello!"},
        {"role": "user", "content": "How are you?"},
        {"role": "assistant", "content": "I am fine, thank you."}
    ]
)

# Now, evaluate the entire session
evaluation_tasks = galtea.evaluation_tasks.create(
    session_id=session.id,
    metrics=[EvaluationMetric(name="coherence"), EvaluationMetric(name="conversation-relevancy")]
)
```

<Warning>
  This method does not support `CustomScoreEvaluationMetric` objects. Attempting to use a custom metric will result in a `ValueError`. For custom scoring, please use the `create_single_turn` method for each turn of the conversation.
</Warning>

## Parameters
<ResponseField name="session_id" type="string" required>
  The ID of the session containing the inference results to be evaluated.
</ResponseField>
<ResponseField name="metrics" type="List[EvaluationMetric]" required>
  A list of standard metrics to use. It is recommended to provide `EvaluationMetric` objects. Optionally, a `model` key can specify an evaluator model (e.g., `EvaluationMetric(name="coherence", model="gpt-4")`).
</ResponseField>
