---
title: 'Create Evaluation Task from Production'
description: "Create an evaluation task from real user interactions in production."
---

## Returns
Returns an EvaluationTask object for the given version and user input.

## Example
```python
evaluation_task = galtea.evaluation_tasks.create_from_production(
    version_id="YOUR_VERSION_ID",
    metrics=["accuracy_v1", "coherence-v1"],
    input=user_query,
    actual_output=model_answer,
    retrieval_context=retrieved_context,
    context=conversation_context,
    conversation_turns=[{"input": past_user_query, "actual_output": past_model_answer}],
    latency=latency,
    usage_info={
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "cache_read_input_tokens": cache_read_input_tokens,
    },
    cost_info={
        "cost_per_input_token": cost_per_input_token,
        "cost_per_output_token": cost_per_output_token,
        "cost_per_cache_read_input_token": cost_per_cache_read_input_token,
    },
)
```
<Info>
  See an example of running evaluation tasks of your product in production in our [Monitor Production Responses to User Queries example](/sdk/examples/monitor-production-responses-to-user-queries).
</Info>

## Parameters
<ResponseField name="version_id" type="string" required>
  The ID of the version of the product you want to evaluate.
</ResponseField>
<ResponseField name="metrics" type="list[string]" required>
  The metrics to use for the evaluation.
  <Note>
    The system will create a task for each metric provided.
  </Note>
</ResponseField>
<ResponseField name="input" type="string" required>
  The real user query that your product handled in production.
</ResponseField>
<ResponseField name="actual_output" type="string" required>
  The actual output produced by the product.
</ResponseField>
<ResponseField name="context" type="string">
  The context that was used to generate the actual output.
</ResponseField>
<ResponseField name="retrieval_context" type="string">
  The context retrieved by your RAG system that was used to generate the actual output.
</ResponseField>
<ResponseField name="conversation_turns" type="list[dict[string, string]]">
  A list of previous conversation turns, each a dictionary with "input" and "actual_output" keys. This is used for evaluating conversational AI.
  Example: `[{"input": "Hello", "actual_output": "Hi there!"}, {"input": "How are you?", "actual_output": "I'm doing well, thanks!"}]`
</ResponseField>
<ResponseField name="latency" type="float">
  Time lapsed (in ms) from the moment the request was sent to the LLM to the moment the response was received.
</ResponseField>
<ResponseField name="usage_info" type="dict[str, float]">
  The token count information of the LLM call. Keys must be snake_case.
</ResponseField>
<ResponseField name="cost_info" type="dict[str, float]">
  The costs of the input and output tokens of the LLM call. Keys must be snake_case.
</ResponseField>
