---
title: 'Create Single-Turn Evaluation Task'
description: "Create an evaluation task for a single-turn interaction, either from a test case or production data."
---

## Returns
Returns a list of [EvaluationTask](/concepts/product/version/session/evaluation/task) objects, one for each metric provided.

## Usage
This method is versatile and can be used for two main scenarios:
1.  **Test-Based Evaluation**: When you provide a `test_case_id`, Galtea evaluates your product's performance against a predefined challenge.
2.  **Production Monitoring**: When you set `is_production=True` and provide an `input`, Galtea logs and evaluates real user interactions.


### Example: Test-Based Evaluation with Standard and Custom Metrics
```python
from galtea import Galtea, CustomScoreEvaluationMetric

# Define your custom metric
class MyAccuracy(CustomScoreEvaluationMetric):
    def __init__(self):
        super().__init__(name="my-accuracy")
    def measure(self, actual_output, **kwargs):
        return 1.0 if "paris" in actual_output.lower() else 0.0

custom_score_accuracy = MyAccuracy()

# If the metric does not exist yet in the platform, you can create it like this:
galtea.metrics.create(name=custom_score_accuracy.name, description='checks for the presence of the keyword "paris" in the output')

# Create evaluation tasks using a mix of standard and custom metrics
evaluation_tasks = galtea.evaluation_tasks.create_single_turn(
    version_id="YOUR_VERSION_ID",
    test_case_id="YOUR_TEST_CASE_ID",
    actual_output="The capital of France is Paris.",
    metrics=[
        "coherence",            # Standard metric
        custom_score_accuracy   # Your custom metric object
    ]
)
```

### Example: Production Monitoring
```python
evaluation_tasks = galtea.evaluation_tasks.create_single_turn(
    version_id="YOUR_VERSION_ID",
    is_production=True,
    input="A real user's query from production",
    actual_output="The model's live response.",
    metrics=["relevance", "non-toxic"],
    retrieval_context="Context retrieved by RAG for this query."
)
```

## Parameters
<ResponseField name="version_id" type="string" required>
  The ID of the version you want to evaluate.
</ResponseField>
<ResponseField name="metrics" type="List[string | CustomScoreEvaluationMetric]" required>
  A list of metrics to use for evaluation. You can provide:
  - Standard metrics as strings (e.g., "coherence").
  - Custom, locally-scored metrics as objects inheriting from `CustomScoreEvaluationMetric`.
</ResponseField>
<ResponseField name="actual_output" type="string" required>
  The actual output produced by the product.
</ResponseField>
<ResponseField name="test_case_id" type="string" optional>
  The ID of the test case to be evaluated. Required for non-production evaluations.
</ResponseField>
<ResponseField name="input" type="string" optional>
  The input text/prompt. Required for production evaluations where no `test_case_id` is provided.
</ResponseField>
<ResponseField name="is_production" type="boolean" optional>
  Set to `True` to indicate the task is from a production environment. Defaults to `False`.
</ResponseField>
<ResponseField name="retrieval_context" type="string" optional>
  The context retrieved by your RAG system that was used to generate the `actual_output`.
</ResponseField>
<ResponseField name="latency" type="float" optional>
  Time in milliseconds from the request to the LLM until the response was received.
</ResponseField>
<ResponseField name="usage_info" type="dict[str, float]" optional>
  Token usage information (e.g., `{"input_tokens": 10, "output_tokens": 5}`).
</ResponseField>
<ResponseField name="cost_info" type="dict[str, float]" optional>
  Cost information for the LLM call.
</ResponseField>
