---
title: 'Evaluation Task Service'
description: "Exploring the Evaluation Task Service API in the Galtea SDK"
icon: "clipboard-check"
iconType: "solid"
---

The Evaluation Task Service in the Galtea SDK allows you to manage [evaluation tasks](/concepts/product/evaluation/task) for assessing your products.
This Service is exposed by the `galtea.evaluation_tasks` object and we will further explore its API down below.

<Info>
  Remember that we will be using the `galtea` object. More information [here](/sdk/api/galtea).
</Info>

## Create Evaluation Task

This method allows you to create an evaluation task.

```python
evaluation_task = galtea.evaluation_tasks.create(
    evaluation_id="YOUR_EVALUATION_ID",
    metrics=["accuracy_v1", "coherence-v1"],
    test_case_id="YOUR_TEST_CASE_ID",
    actual_output="Barcelona",
)
```

<Info>
  See an example of running evaluation tasks in our [Run Evaluations example](/sdk/examples/create-evaluation).
</Info>

<ResponseField name="evaluation_id" type="string" required>
  The ID of the evaluation you want to create a task for.
</ResponseField>

<ResponseField name="metrics" type="list[string]" required>
  The metrics to use for the evaluation.
  <Note>
    The system will create a task for each metric given.
  </Note>
</ResponseField>

<ResponseField name="test_case_id" type="string" required>
  The ID of the test case you want to use for the evaluation task.
</ResponseField>

<ResponseField name="actual_output" type="string" required>
  The actual output produced by the product.
</ResponseField>

<ResponseField name="retrieval_context" type="string">
  The context retrieved by your RAG system that was used to generate the actual output.
  This allows evaluation of retrieval quality in RAG systems.
  <Note>
    Including retrieval context enables more comprehensive evaluation of RAG systems,
    allowing for assessment of both retrieval and generation capabilities.
  </Note>
</ResponseField>

<ResponseField name="conversation turns" type="list[dict[string, string]]">
  The conversation turns that were used to generate the actual output as additional context for the model. Basically the conversation history (without the current user_query / model_answer).
  This allows evaluation of the model's performance in a conversational context.
  There is a expected format for the conversation turns:
  ```json
  [
    {"input": "What is the capital of France?", "actual_output": "Paris"},
    {"input": "What is the population of it?", "actual_output": "two million"}
  ]
  ```

  <Note>
    Currently, this parameter is useful for these kind of default metrics available in the platform:
    - **Role Adherence**: Measures how well the actual output adheres to a specified role.
    - **Knowledge Retention**: Assesses the model's ability to retain and use information from previous turns in the conversation.
    - **Conversation Completeness**: Evaluates whether the conversation has reached a natural and informative conclusion.
    - **Conversation Relevancy**: Assesses whether each turn in the conversation is relevant to the ongoing topic and user needs.
 </Note>
</ResponseField>

<ResponseField name="scores" type="list[float | None]">
  Precomputed scores for the evaluation tasks, corresponding to the provided metrics. Must be a list of the same size as the `metrics` parameter, containing numbers between 0 and 1 or `None` values.
    - Providing scores bypasses platform-based evaluation, storing the provided scores for later analysis. Useful for **custom metrics**.
    - Use `None` as a placeholder for metrics that should be evaluated by the platform (e.g., `[0.5, None, 0.9]`).
  <Note>
    This field is automatically populated by the system and should not be provided manually unless you are evaluating custom metrics.
  </Note>
</ResponseField>

<ResponseField name="latency" type="float">
  Time lapsed (in ms) from the moment the request was sent to the LLM to the moment the response was received.
  <Note>
    You can get the latency in milliseconds with this code:
    ```python
      timeBeforeCall = datetime.now()
      model_answer = CALL_TO_YOUR_MODEL(test_case.input)
      timeAfterCall = datetime.now()
      latency=(timeAfterCall - timeBeforeCall).total_seconds() * 1000
    ``` 
  </Note>
</ResponseField>

<ResponseField name="usage_info" type="dict[string, float]">
  The token count information of the LLM call, possible keys include:
    - **input_tokens**: The number of input tokens used in the LLM call.
    - **output_tokens**: The number of output tokens generated by the LLM call.
    - **cache_read_input_tokens**: The number of tokens read by the model from cache.
  <Note>
    The token count is calculated based on the LLM provider's tokenization method and it's usually returned in the response of the endpoint.
  </Note>
</ResponseField>

<ResponseField name="cost_info" type="dict[string, float]">
  The costs of the input and output tokens of the LLM call, possible keys include:
    - **cost_per_input_token**: The cost per input token read by the LLM.
    - **cost_per_output_token**: The cost per output token generated by the LLM.
    - **cost_per_cache_read_input_token**: The cost per input token read from cache.
  <Note>
    The cost is calculated based on the pricing model of the LLM provider, ensure to check it and input the real numbers for accurate estimations.
  </Note>
  <Warning>
    If the cost information is properly configured in the [Model](/concepts/model) selected by the used [Version](/concepts/product/version), the system will automatically calculate the cost of the evaluation task and the `cost_info` does not need to be provided.

    However, if the `cost_info` is provided, it will override the system's calculation.
  </Warning>
</ResponseField>

<Warning>
  It is possible to create evaluation tasks with customized `input`, `expected_output`, and `context` fields by following [this example](/sdk/examples/create-evaluation#customized-test-cases-for-evaluation-tasks).

  However, this is not recommended for most use cases, as it reduces the consistency and comparability of evaluation results.
</Warning>

## Monitoring User Interactions in Production

If you want to monitor the user interactions for your application in production, you can do it simply by creating an evaluation task from the `input`s that your users are introducing to your system.
This way, you can evaluate your product using real-world data and continuously improve its performance.
The benefits of this approach include:
- **Real-world data**: You can evaluate your product using real-world data, which can provide more accurate insights into its performance.
- **Continuous improvement**: By continuously evaluating your product in production with real data, you can better identify areas for improvement and make necessary adjustments.
- **Performance tracking**: You can track the performance of your product over time, allowing you to identify trends and make data-driven decisions.
- **Analysis of inputs and outputs**: You can analyze the inputs and outputs of your product in a real-world context, which can help you understand how it is being used and identify areas for improvement.

```python
evaluation_task = galtea.evaluation_tasks.create_from_production(
    version_id="YOUR_VERSION_ID",
    metrics=["accuracy_v1", "coherence-v1"],
    input=user_query,
    actual_output=model_answer,
    retrieval_context=retrieved_context,
    context=conversation_context,
    conversation_turns=[{"input": past_user_query, "actual_output": past_model_answer}],
    latency=latency,
    usage_info={
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "cache_read_input_tokens": cache_read_input_tokens,
    },
    cost_info={
        "cost_per_input_token": cost_per_input_token,
        "cost_per_output_token": cost_per_output_token,
        "cost_per_cache_read_input_token": cost_per_cache_read_input_token,
    },
)
```

<ResponseField name="version_id" type="string" required>
  The ID of the version of the product you want to evaluate.
</ResponseField>

<ResponseField name="metrics" type="list[string]" required>
  The metrics to use for the evaluation.
  <Note>
    The system will create a task for each metric given.
  </Note>
</ResponseField>

<ResponseField name="input" type="string" required>
  The real user query that your product handled in production.
</ResponseField>

<ResponseField name="actual_output" type="string" required>
  The actual output produced by the product.
</ResponseField>

<ResponseField name="context" type="string">
  The context that was used to generate the actual output.
</ResponseField>

<ResponseField name="retrieval_context" type="string">
  The context retrieved by your RAG system that was used to generate the actual output.
</ResponseField>

<ResponseField name="conversation turns" type="list[dict[string, string]]">
  The conversation turns that were used to generate the actual output as additional context for the model. Basically the conversation history (without the current user_query / model_answer).
  This allows evaluation of the model's performance in a conversational context.
  There is a expected format for the conversation turns:
  ```json
  [
    {"input": "What is the capital of France?", "actual_output": "Paris"},
    {"input": "What is the population of it?", "actual_output": "two million"}
  ]
  ```
  <Note>
    Currently, this parameter is useful for these kind of default metrics available in the platform:
    - **Role Adherence**: Measures how well the actual output adheres to a specified role.
    - **Knowledge Retention**: Assesses the model's ability to retain and use information from previous turns in the conversation.
    - **Conversation Completeness**: Evaluates whether the conversation has reached a natural and informative conclusion.
    - **Conversation Relevancy**: Assesses whether each turn in the conversation is relevant to the ongoing topic and user needs.
 </Note>
</ResponseField>

<ResponseField name="latency" type="float">
  Time lapsed (in ms) from the moment the request was sent to the LLM to the moment the response was received.
  <Note>
    You can get the latency in milliseconds with this code:
    ```python
      timeBeforeCall = datetime.now()
      model_answer = CALL_TO_YOUR_MODEL(test_case.input)
      timeAfterCall = datetime.now()
      latency=(timeAfterCall - timeBeforeCall).total_seconds() * 1000
    ``` 
  </Note>
</ResponseField>

<ResponseField name="usage_info" type="dict[string, float]">
  The token count information of the LLM call, possible keys include:
    - **input_tokens**: The number of input tokens used in the LLM call.
    - **output_tokens**: The number of output tokens generated by the LLM call.
    - **cache_read_input_tokens**: The number of tokens read by the model from cache.
  <Note>
    The token count is calculated based on the LLM provider's tokenization method and it's usually returned in the response of the endpoint.
  </Note>
</ResponseField>

<ResponseField name="cost_info" type="dict[string, float]">
  The costs of the input and output tokens of the LLM call, possible keys include:
    - **cost_per_input_token**: The cost per input token read by the LLM.
    - **cost_per_output_token**: The cost per output token generated by the LLM.
    - **cost_per_cache_read_input_token**: The cost per input token read from cache.
  <Note>
    The cost is calculated based on the pricing model of the LLM provider, ensure to check it and input the real numbers for accurate estimations.
  </Note>
  <Warning>
    If the cost information is properly configured in the [Model](/concepts/model) selected by the used [Version](/concepts/product/version), the system will automatically calculate the cost of the evaluation task and the `cost_info` does not need to be provided.

    However, if the `cost_info` is provided, it will override the system's calculation.
  </Warning>
</ResponseField>


## Listing Evaluation Tasks

This method allows you to list all evaluation tasks associated with a specific evaluation.

```python
evaluation_tasks = galtea.evaluation_tasks.list(evaluation_id="YOUR_EVALUATION_ID")
```

<ResponseField name="evaluation_id" type="string" required>
  The ID of the evaluation for which you want to list tasks.
</ResponseField>

<ResponseField name="offset" type="int">
  The number of tasks to skip before starting to collect the result set.
</ResponseField>

<ResponseField name="limit" type="int">
  The maximum number of tasks to return.
</ResponseField>

## Retrieving Evaluation Task

This method allows you to retrieve a specific evaluation task by its ID.

```python
evaluation_task = galtea.evaluation_tasks.get(task_id="YOUR_TASK_ID")
```

<ResponseField name="evaluation_task_id" type="string" required>
  The ID of the task you want to retrieve.
</ResponseField>

## Deleting Evaluation Task

This method allows you to delete a specific evaluation task by its ID.

```python
galtea.evaluation_tasks.delete(task_id="YOUR_TASK_ID")
```

<ResponseField name="evaluation_task_id" type="string" required>
  The ID of the task you want to delete.
</ResponseField>

