---
title: 'Evaluation Task Service'
description: "Exploring the Evaluation Task Service API in the Galtea SDK"
icon: "clipboard-check"
iconType: "solid"
---

The Evaluation Task Service in the Galtea SDK allows you to manage [evaluation tasks](/concepts/product/evaluation/task) for assessing your products.
This Service is exposed by the `galtea.evaluation_tasks` object and we will further explore its API down below.

<Info>
  Remember that we will be using the `galtea` object. More information [here](/sdk/api/galtea).
</Info>

## Create Evaluation Task

This method allows you to create an evaluation task.

```python
evaluation_task = galtea.evaluation_tasks.create(
    evaluation_id="YOUR_EVALUATION_ID",
    metrics=["accuracy_v1", "coherence-v1"],
    test_case_id="YOUR_TEST_CASE_ID",
    actual_output="Barcelona",
)
```

<Info>
  See an example of running evaluation tasks in our [Run Evaluations example](/sdk/examples/create-evaluation).
</Info>

<ResponseField name="evaluation_id" type="string" required>
  The ID of the evaluation you want to create a task for.
</ResponseField>

<ResponseField name="metrics" type="list[string]" required>
  The metrics to use for the evaluation.
  <Note>
    The system will create a task for each metric given.
  </Note>
</ResponseField>

<ResponseField name="test_case_id" type="string" required>
  The ID of the test case you want to use for the evaluation task.
</ResponseField>

<ResponseField name="actual_output" type="string" required>
  The actual output produced by the product.
</ResponseField>

<ResponseField name="latency" type="float">
  Time lapsed (in ms) from the moment the request was sent to the LLM to the moment the response was received.
  <Note>
    You can get the latency in milliseconds with this code:
    ```python
      timeBeforeCall = datetime.now()
      model_answer = "Android 8.0" # Replace with the actual model answer by calling it with the test case input and the test case context
      timeAfterCall = datetime.now()
      latency=(timeAfterCall - timeBeforeCall).total_seconds() * 1000
    ``` 
  </Note>
</ResponseField>

<ResponseField name="usage_info" type="dict[string, float]">
  The token count information of the LLM call, possible keys include:
    - **input_tokens**: The number of input tokens used in the LLM call.
    - **output_tokens**: The number of output tokens generated by the LLM call.
    - **cache_read_input_tokens**: The number of tokens read by the model from cache.
  <Note>
    The token count is calculated based on the LLM provider's tokenization method and it's usually returned in the response of the endpoint.
  </Note>
</ResponseField>

<ResponseField name="cost_info" type="dict[string, float]">
  The costs of the input and output tokens of the LLM call, possible keys include:
    - **cost_per_input_token**: The cost per input token read by the LLM.
    - **cost_per_output_token**: The cost per output token generated by the LLM.
    - **cost_per_cache_read_input_token**: The cost per input token read from cache.
  <Note>
    The cost is calculated based on the pricing model of the LLM provider, ensure to check it and input the real numbers for accurate estimations.
  </Note>
</ResponseField>

<Warning>
  It is possible to create evaluation tasks with customized `input`, `expected_output`, and `context` fields by following [this example](/sdk/examples/create-evaluation#customized-test-cases-for-evaluation-tasks).

  However, this is not recommended for most use cases, as it reduces the consistency and comparability of evaluation results.
</Warning>


## Listing Evaluation Tasks

This method allows you to list all evaluation tasks associated with a specific evaluation.

```python
evaluation_tasks = galtea.evaluation_tasks.list(evaluation_id="YOUR_EVALUATION_ID")
```

<ResponseField name="evaluation_id" type="string" required>
  The ID of the evaluation for which you want to list tasks.
</ResponseField>

<ResponseField name="offset" type="int">
  The number of tasks to skip before starting to collect the result set.
</ResponseField>

<ResponseField name="limit" type="int">
  The maximum number of tasks to return.
</ResponseField>

## Retrieving Evaluation Task

This method allows you to retrieve a specific evaluation task by its ID.

```python
evaluation_task = galtea.evaluation_tasks.get(task_id="YOUR_TASK_ID")
```

<ResponseField name="evaluation_task_id" type="string" required>
  The ID of the task you want to retrieve.
</ResponseField>

## Deleting Evaluation Task

This method allows you to delete a specific evaluation task by its ID.

```python
galtea.evaluation_tasks.delete(task_id="YOUR_TASK_ID")
```

<ResponseField name="evaluation_task_id" type="string" required>
  The ID of the task you want to delete.
</ResponseField>

