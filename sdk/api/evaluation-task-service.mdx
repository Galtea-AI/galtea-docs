---
title: 'Evaluation Task Service'
description: "Exploring the Evaluation Task Service API in the Galtea SDK"
icon: "clipboard-check"
iconType: "solid"
---

The Evaluation Task Service in the Galtea SDK allows you to manage [evaluation tasks](/concepts/product/evaluation/task) for assessing your products.
- This service is exposed via the `galtea.evaluation_tasks` object; see its API below.

<Info>
  Remember that we will be using the `galtea` object. More information [here](/sdk/api/galtea).
</Info>

## Create Evaluation Task

This method allows you to create an evaluation task.

```python
evaluation_task = galtea.evaluation_tasks.create(
    evaluation_id="YOUR_EVALUATION_ID",
    metrics=["accuracy_v1", "coherence-v1"],
    test_case_id="YOUR_TEST_CASE_ID",
    actual_output="Barcelona",
)
```

<Info>
  See an example of running evaluation tasks in our [Run Evaluations example](/sdk/examples/create-evaluation).
</Info>

<ResponseField name="evaluation_id" type="string" required>
  The ID of the evaluation you want to create a task for.
</ResponseField>

<ResponseField name="metrics" type="list[string]" required>
  The metrics to use for the evaluation.
  <Note>
    The system will create a task for each metric provided.
  </Note>
</ResponseField>

<ResponseField name="test_case_id" type="string" required>
  The ID of the test case you want to use for the evaluation task. If `test_case_id` is provided, the `input`, `expected_output`, and `context` fields will be sourced from the specified test case. Providing these parameters directly alongside `test_case_id` will result in a deprecation warning from the SDK and is generally discouraged for consistency.
</ResponseField>

<ResponseField name="actual_output" type="string" required>
  The actual output produced by the product.
</ResponseField>

<ResponseField name="retrieval_context" type="string">
  The context retrieved by your RAG system that was used to generate the actual output.
  This allows evaluation of retrieval quality in RAG systems.
  <Note>
    Including retrieval context enables more comprehensive evaluation of RAG systems,
    allowing for assessment of both retrieval and generation capabilities.
  </Note>
</ResponseField>

<ResponseField name="conversation turns" type="list[dict[string, string]]">
  A list of previous conversation turns, each a dictionary with "input" and "actual_output" keys. This is used for evaluating conversational AI.
  Example: `[{"input": "Hello", "actual_output": "Hi there!"}, {"input": "How are you?", "actual_output": "I'm doing well, thanks!"}]`
  Currently, this parameter is primarily used with default conversational metrics like Role Adherence, Knowledge Retention, etc.
</ResponseField>

<ResponseField name="scores" type="list[float | None]">
  A list of pre-calculated scores, corresponding to each metric in the `metrics` list. Each score must be between 0.0 and 1.0. Use `None` if you want Galtea to evaluate a specific metric. This is primarily for custom metrics evaluated externally. The list length must match the `metrics` list length.
</ResponseField>

<ResponseField name="latency" type="float">
  Time lapsed (in ms) from the moment the request was sent to the LLM to the moment the response was received.
  <Note>
    You can get the latency in milliseconds with this code:
    ```python
      timeBeforeCall = datetime.now()
      model_answer = CALL_TO_YOUR_MODEL(test_case.input)
      timeAfterCall = datetime.now()
      latency=(timeAfterCall - timeBeforeCall).total_seconds() * 1000
    ``` 
  </Note>
</ResponseField>

<ResponseField name="usage_info" type="dict[string, float]">
  The token count information of the LLM call, possible keys include:
    - **input_tokens**: The number of input tokens used in the LLM call.
    - **output_tokens**: The number of output tokens generated by the LLM call.
    - **cache_read_input_tokens**: The number of tokens read by the model from cache.
  <Note>
    The token count is calculated based on the LLM provider's tokenization method and it's usually returned in the response of the endpoint.
  </Note>
</ResponseField>

<ResponseField name="cost_info" type="dict[string, float]">
  The costs of the input and output tokens of the LLM call, possible keys include:
    - **cost_per_input_token**: The cost per input token read by the LLM.
    - **cost_per_output_token**: The cost per output token generated by the LLM.
    - **cost_per_cache_read_input_token**: The cost per input token read from cache.
  <Note>
    The cost is calculated based on the pricing model of the LLM provider, ensure to check it and input the real numbers for accurate estimations.
  </Note>
  <Warning>
    If the cost information is properly configured in the [Model](/concepts/model) selected by the used [Version](/concepts/product/version), the system will automatically calculate the cost of the evaluation task and the `cost_info` does not need to be provided.
    However, if the `cost_info` is provided, it will override the system's calculation.
  </Warning>
</ResponseField>

<Warning>
  The `input`, `expected_output`, and `context` parameters are deprecated when `test_case_id` is provided. Prefer using `test_case_id` to link to a predefined test case. Providing these directly may lead to inconsistencies in tracking and is intended for specific ad-hoc scenarios or evaluation of production data not tied to a formal test.
</Warning>

<Warning>
  It is possible to create evaluation tasks with customized `input`, `expected_output`, and `context` fields by following [this example](/sdk/examples/create-evaluation#customized-test-cases-for-evaluation-tasks).

  However, this is not recommended for most use cases, as it reduces the consistency and comparability of evaluation results.
</Warning>

## Monitoring User Interactions in Production

- If you want to monitor user interactions in production, do so simply by creating an evaluation task from the inputs that your users send to your system.
This way, you can evaluate your product using real-world data and continuously improve its performance.
The benefits of this approach include:
- **Real-world data**: You can evaluate your product using real-world data, which can provide more accurate insights into its performance.
- **Continuous improvement**: By continuously evaluating your product in production with real data, you can better identify areas for improvement and make necessary adjustments.
- **Performance tracking**: You can track the performance of your product over time, allowing you to identify trends and make data-driven decisions.
- **Analysis of inputs and outputs**: You can analyze the inputs and outputs of your product in a real-world context, which can help you understand how it is being used and identify areas for improvement.

<Info>
  See an example of running evaluation tasks of your product in production in our [Monitor Production Responses to User Queries example](/sdk/examples/monitor-production-responses-to-user-queries).
</Info>

```python
evaluation_task = galtea.evaluation_tasks.create_from_production(
    version_id="YOUR_VERSION_ID",
    metrics=["accuracy_v1", "coherence-v1"],
    input=user_query,
    actual_output=model_answer,
    retrieval_context=retrieved_context,
    context=conversation_context,
    conversation_turns=[{"input": past_user_query, "actual_output": past_model_answer}],
    latency=latency,
    usage_info={
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "cache_read_input_tokens": cache_read_input_tokens,
    },
    cost_info={
        "cost_per_input_token": cost_per_input_token,
        "cost_per_output_token": cost_per_output_token,
        "cost_per_cache_read_input_token": cost_per_cache_read_input_token,
    },
)
```

<ResponseField name="version_id" type="string" required>
  The ID of the version of the product you want to evaluate.
</ResponseField>

<ResponseField name="metrics" type="list[string]" required>
  The metrics to use for the evaluation.
  <Note>
    The system will create a task for each metric provided.
  </Note>
</ResponseField>

<ResponseField name="input" type="string" required>
  The real user query that your product handled in production.
</ResponseField>

<ResponseField name="actual_output" type="string" required>
  The actual output produced by the product.
</ResponseField>

<ResponseField name="context" type="string">
  The context that was used to generate the actual output.
</ResponseField>

<ResponseField name="retrieval_context" type="string">
  The context retrieved by your RAG system that was used to generate the actual output.
</ResponseField>

<ResponseField name="conversation turns" type="list[dict[string, string]]">
  A list of previous conversation turns, each a dictionary with "input" and "actual_output" keys. This is used for evaluating conversational AI.
  Example: `[{"input": "Hello", "actual_output": "Hi there!"}, {"input": "How are you?", "actual_output": "I'm doing well, thanks!"}]`
  Currently, this parameter is primarily used with default conversational metrics like Role Adherence, Knowledge Retention, etc.
  <Info>
    See an example of evaluating conversations in our [Monitor Production Responses to User Queries example](/sdk/examples/monitor-production-responses-to-user-queries#evaluating-conversations).
  </Info>
</ResponseField>

<ResponseField name="latency" type="float">
  Time lapsed (in ms) from the moment the request was sent to the LLM to the moment the response was received.
  <Note>
    You can get the latency in milliseconds with this code:
    ```python
      timeBeforeCall = datetime.now()
      model_answer = CALL_TO_YOUR_MODEL(test_case.input)
      timeAfterCall = datetime.now()
      latency=(timeAfterCall - timeBeforeCall).total_seconds() * 1000
    ``` 
  </Note>
</ResponseField>

<ResponseField name="usage_info" type="dict[string, float]">
  The token count information of the LLM call, possible keys include:
    - **input_tokens**: The number of input tokens used in the LLM call.
    - **output_tokens**: The number of output tokens generated by the LLM call.
    - **cache_read_input_tokens**: The number of tokens read by the model from cache.
  <Note>
    The token count is calculated based on the LLM provider's tokenization method and it's usually returned in the response of the endpoint.
  </Note>
</ResponseField>

<ResponseField name="cost_info" type="dict[string, float]">
  The costs of the input and output tokens of the LLM call, possible keys include:
    - **cost_per_input_token**: The cost per input token read by the LLM.
    - **cost_per_output_token**: The cost per output token generated by the LLM.
    - **cost_per_cache_read_input_token**: The cost per input token read from cache.
  <Note>
    The cost is calculated based on the pricing model of the LLM provider, ensure to check it and input the real numbers for accurate estimations.
  </Note>
  <Warning>
    If the cost information is properly configured in the [Model](/concepts/model) selected by the used [Version](/concepts/product/version), the system will automatically calculate the cost of the evaluation task and the `cost_info` does not need to be provided.
    However, if the `cost_info` is provided, it will override the system's calculation.
  </Warning>
</ResponseField>


## Listing Evaluation Tasks

This method allows you to list all evaluation tasks associated with a specific evaluation.

```python
evaluation_tasks = galtea.evaluation_tasks.list(evaluation_id="YOUR_EVALUATION_ID")
```

<ResponseField name="evaluation_id" type="string" required>
  The ID of the evaluation for which you want to list tasks.
</ResponseField>

<ResponseField name="offset" type="int">
  The number of tasks to skip before starting to collect the result set.
</ResponseField>

<ResponseField name="limit" type="int">
  The maximum number of tasks to return.
</ResponseField>

## Retrieving Evaluation Task

This method allows you to retrieve a specific evaluation task by its ID.

```python
evaluation_task = galtea.evaluation_tasks.get(task_id="YOUR_TASK_ID")
```

<ResponseField name="evaluation_task_id" type="string" required>
  The ID of the task you want to retrieve.
</ResponseField>

## Deleting Evaluation Task

This method allows you to delete a specific evaluation task by its ID.

```python
galtea.evaluation_tasks.delete(task_id="YOUR_TASK_ID")
```

<ResponseField name="evaluation_task_id" type="string" required>
  The ID of the task you want to delete.
</ResponseField>

