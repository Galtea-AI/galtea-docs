---
title: "Metrics Overview"
description: "Ensure that your product works as intended"
"icon": "chart-bar"
---

## Understanding the performance of your products with relevant metrics

Evaluating the performance, robustness, and safety of LLM-based products requires more than traditional metrics. It demands a broader and more contextual approach—one that incorporates domain-specific knowledge and business relevance to truly capture how these systems behave in real-world settings. The Galtea platform supports this by offering a comprehensive set of both default and customizable metrics. Only through a diverse and targeted evaluation framework can developers gain a meaningful understanding of their product’s strengths, limitations, and areas for improvement. The sections below outline our methodology for constructing and applying these metrics.

## Conceptual categories

At Galtea, we use two types of metrics to evaluate large language model (LLM) outputs: **deterministic** and **non-deterministic**.

- **Deterministic metrics** are rule-based and computed using strict logic, such as SQL queries or structural checks. These include things like answer format validation, presence of required fields, or exact string matches. Their results are consistent and reproducible. See more in our documentation: [Custom Metrics](/sdk/examples/create-evaluation-with-custom-metric)
- **Non-deterministic metrics** are based on the methodology of LLM-as-a-judge. For the user defined metrics we use the **G-Eval** framework ([G-Eval paper](https://arxiv.org/abs/2303.16634)). These leverage LLMs themselves to evaluate aspects like factuality, coherence, helpfulness, or tone. Because they use LLMs under-the-hood, results can vary slightly. Galtea also maintains other LLM-as-a-judge metrics, for more straightforward cases of evaluation: Faithfulness, Answer Relevancy, Contextual Precision, Contextual Recall, Contextual Relevancy for quality; Unbiased, Non-toxic for Red teaming.

## List of metrics available in the Galtea Platform

| Metric | Category | Description |
| --- | --- | --- |
| **Answer Relevancy** | RAG | Measures the quality of your RAG pipeline's generator by evaluating how relevant the actual_output of your LLM application is compared to the provided input. |
| **Faithfulness** | RAG | Measures the quality of your RAG pipeline's generator by evaluating whether the actual_output factually aligns with the contents of your retrieval_context |
| **Contextual Precision** | RAG | Measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval_context that are relevant to the given input are ranked higher than irrelevant ones. |
| **Contextual Recall** | RAG | Measures the quality of your RAG pipeline's retriever by evaluating the extent of which the retrieval_context aligns with the expected_output. |
| **Contextual Relevancy** | RAG | Measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval_context for a given input. |
| **Role Adherence** | Conversational | Determines whether your LLM chatbot is able to adhere to its given role throughout a conversation. |
| **Conversation Completeness** | Conversational | Determines whether your LLM chatbot is able to complete an end-to-end conversation by satisfying user needs throughout a conversation. |
| **Conversation Relevancy** | Conversational | Determines whether your LLM chatbot is able to consistently generate relevant responses throughout a conversation. |
| **Knowledge Retention** | Conversational | Assesses the ability of your LLM chatbot to retain factual information presented throughout a conversation. |
| **Non-Toxic** | Red Teaming | Determines whether the responses of your LLM based product responds are free of toxic language. |
| **Unbiased** | Red Teaming | Determine whether your LLM output is free of gender, racial, or political bias. |

Choosing the right set of metrics is essential for aligning your LLM product with user expectations, compliance requirements, and domain-specific goals. Whether you are fine-tuning a RAG pipeline, monitoring a production chatbot, or running red teaming tests, the Galtea platform offers an extensible framework for integrating evaluation directly into your development and deployment workflows.

To get started with building your own custom evaluations, visit our [SDK Usage Guide](sdk/usage). If you’d like to see how these metrics are used in practice, explore our [Example Projects](sdk/examples/create-evaluation). For deeper insight into how your metrics are created be it LLM-as-a-judge metrics or custom, see [Metrics Type reference](concepts/metric-type). You can further delve deep into the details of each metric in the following sections.