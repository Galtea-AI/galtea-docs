---
title: "G-Eval"
description: "A flexible, LLM-as-a-judge evaluation framework for building custom, human-aligned metrics."
icon: "cogs"
iconType: "solid"
---

**G-Eval** is the most versatile evaluation framework available in the Galtea platform. It leverages LLM-as-a-judge with chain-of-thought reasoning to evaluate model outputs against any user-defined criteria. This approach enables accurate, scalable, and human-aligned assessments across a wide range of use cases—whether factuality, helpfulness, stylistic control, or domain-specific compliance.

G-Eval was originally introduced in the paper ["NLG Evaluation using GPT-4 with Better Human Alignment"](https://arxiv.org/abs/2303.16634), and is one of the most effective techniques for custom, task-specific evaluation.

---

## Evaluation Parameters

To compute a `geval` metric, you must provide at least:

- **`input`**: The user’s query or instruction.
- **`actual_output`**: The response generated by your LLM-based application.

Depending on your use case, you may also include:

- **`expected_output`**: A reference or ground truth response (if your evaluation compares outputs to gold standards).
- **`context`**: Additional background or retrieved information used by the model.

These parameters should exists as a part of your [Test Case](/concepts/product/test/case) and are passed to the [Evaluation Task](/concepts/product/evaluation/task) automatically when an Evaluation Task is created.

---

## Evaluation Criteria and Steps

The core of G-Eval is its support for natural language evaluation criteria and structured evaluation steps.

- **Evaluation Criteria**: A free-text description of what constitutes a good response (e.g., *"The answer should be concise, accurate, and relevant to the user question."*).
- **Evaluation Steps**: An optional list of reasoning steps that the evaluator should follow when scoring the output.

<Note>You can only use either **Evaluation Criteria** or  **Evaluation Steps** when creating the metric. </Note>

If you do not provide `evaluation_steps`, G-Eval will auto-generate them from your criteria using a chain-of-thought prompting strategy. This makes setup fast, but also introduces some variability.

When you *do* provide `evaluation_steps`, the metric becomes more reliable and deterministic across runs—ideal for production use and reproducibility.


---

## How Is It Calculated?

G-Eval follows a structured two-step evaluation process:

1. **Chain-of-Thought Generation** *(optional)*:
   - If no `evaluation_steps` are provided, G-Eval generates them from the input `criteria` using a chain-of-thought prompting approach.

2. **Score Computation**:
   - G-Eval constructs a prompt using the evaluation steps and all available test case parameters.
   - The LLM is asked to rate the response on a **1–5 scale**, where **5 is best**.
   - The final metric score (between 0 and 1) is obtained by normalizing the LLM’s scoring output using token probabilities and a weighted average strategy.

This approach mimics the reasoning process a human evaluator might follow, but at scale and with high consistency.

---

## When to Use G-Eval

Use G-Eval when:

- You need **custom metrics** for your domain or product.
- Your evaluation logic cannot be captured by simple string comparisons or rule-based scoring.
- You want a **reliable LLM-as-a-judge** method with interpretable and tunable evaluation steps.

Common use cases include:
- Checking correctness of formatting specifications
- Stylistic adherence in marketing copy
- Ensuring detailed prompt adherence for relevant restrictions

---
