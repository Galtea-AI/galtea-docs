---
title: "Evaluation Task"
description: "The assessment of a specific test case using a metric type"
icon: "clipboard-check"
iconType: "solid"
---

import EvaluationCard from '/snippets/cards/evaluation-card.mdx';
import MetricTypeCard from '/snippets/cards/metric-type-card.mdx';
import SDKEvaluationTaskServiceCard from '/snippets/cards/sdk-evaluation-task/service-card.mdx';
import TestCard from '/snippets/cards/test-card.mdx';
import TestCaseCard from '/snippets/cards/test-case-card.mdx';

## What is an Evaluation Task?

An evaluation task in Galtea represents the assessment of a single [test case](/concepts/product/test/case) from a [test](/concepts/product/test) using a specific [evaluation criteria](/concepts/metric-type). Multiple evaluation tasks make up an [evaluation](/concepts/product/evaluation).

<Note>Evaluation tasks don't perform inference on the LLM product themselves. Rather, they evaluate outputs that have already been generated. You should perform inference on your product first, then trigger the evaluation task with both the input (provided by the test case) and the resulting output.</Note>

## Task Lifecycle

Evaluation tasks follow a specific lifecycle:

<Steps>
  <Step title="Inference">Perform inference on the LLM product to generate an output using the test case's input and context</Step>
  <Step title="Creation">[Trigger an evaluation task](/sdk/examples/create-evaluation). It will appear in the Evaluation's details page with the status "pending"</Step>
  <Step title="Processing">Galtea's evaluation system processes the task using evaluation criteria of the selected metric type</Step>
  <Step title="Completion">Once processed, the status changes to "completed" and the [results](#task-results-properties) are available</Step>
</Steps>

## Related Concepts

<CardGroup cols={2}>
  <TestCard />
  <TestCaseCard />
  <EvaluationCard />
  <MetricTypeCard />
</CardGroup>

## SDK Integration

<SDKEvaluationTaskServiceCard />

## Task Creation Properties

To trigger an evaluation task in Galtea, you can provide the following information:

<ResponseField name="Evaluation" type="ID (text)" required>
  The unique identifier of the evaluation that the task is part of.
</ResponseField>

<ResponseField name="Test Case" type="ID (text)" required>
  The unique identifier of the test case to be evaluated.
</ResponseField>

<ResponseField name="Input" type="string" deprecated>
  The user query that your model needs to answer. Deprecated, it is now part of the test case.
</ResponseField>

<ResponseField name="Expected Output" type="string" deprecated>
  Expected output for the evaluation task. Deprecated, it is now part of the test case.
</ResponseField>

<ResponseField name="Context" type="string" deprecated>
  Additional data provided to your LLM application. Deprecated, it is now part of the test case.
</ResponseField>

<ResponseField name="Actual Output" type="String" required>
  The actual output produced by the product's version. **Example**: "The iPhone 16 costs $950."
</ResponseField>

<ResponseField name="Retrieval Context" type="string">
  The context retrieved by your RAG system that was used to generate the actual output.
  <Note>
    Including retrieval context enables more comprehensive evaluation of RAG systems, allowing for assessment of both retrieval and generation capabilities.
  </Note>
</ResponseField>

<ResponseField name="Metric Type" type="ID (text)" required>
  The unique identifier of the metric type to be used in the evaluation task.
  <Info>The SDK allows you to use the metric type's name instead of the ID.</Info>
</ResponseField>

<ResponseField name="Conversation Turns" type="List[Dict[str, str]]">
  A list of previous conversation turns, each a dictionary with "input" and "actual_output" keys. This is used for evaluating conversational AI. If `test_case_id` is provided, this will be sourced from the test case. Can be provided for production monitoring evaluations.
  Example: `[{"input": "Hello", "actual_output": "Hi there!"}, {"input": "How are you?", "actual_output": "I'm doing well, thanks!"}]`
  Refer to the [SDK documentation](/sdk/api/evaluation-task/service#param-conversation-turns) for more details.
</ResponseField>

<ResponseField name="Latency" type="float">
  Time lapsed (in ms) from the moment the request was sent to the LLM to the moment the response was received.
  <Note>
    You can get the latency in milliseconds with this code:
    ```python
      from datetime import datetime
      timeBeforeCall = datetime.now()
      model_answer = CALL_TO_YOUR_MODEL(test_case.input) # Your model call
      timeAfterCall = datetime.now()
      latency=(timeAfterCall - timeBeforeCall).total_seconds() * 1000
    ```
  </Note>
</ResponseField>

<ResponseField name="Usage Info" type="Object">
  Token count information of the LLM call. Use snake_case keys: `input_tokens`, `output_tokens`, `cache_read_input_tokens`.
  <Note>
    Token counts are calculated based on the LLM provider's tokenization method and are usually returned in the response of the endpoint.
  </Note>
</ResponseField>

<ResponseField name="Cost Info" type="Object">
  The costs associated with the LLM call. Use snake_case keys. May include:
  - **cost_per_input_token**: Cost for each input token read by the LLM.
  - **cost_per_output_token**: Cost for each output token generated by the LLM.
  - **cost_per_cache_read_input_token**: Cost for each input token read from cache.
  <Note>
    Costs are calculated based on the pricing model of the LLM provider.
  </Note>
  <Warning>
    If cost information is properly configured in the [Model](/concepts/model) selected by the [Version](/concepts/product/version), the system will automatically calculate the cost of the evaluation task and you don't need to provide this information. However, if provided, it will override the system's calculation.
  </Warning>
</ResponseField>

### Monitoring Production Data
Galtea SDK also supports creating evaluation tasks directly from your production environment using the `create_from_production` method. This is useful for ongoing monitoring and analysis of real user interactions. For detailed usage, refer to the [`create_from_production` documentation](/sdk/api/evaluation-task/service#monitoring-user-interactions-in-production).

## Task Results Properties

Once an evaluation task is completed, you can access the following information:

<ResponseField name="Status" type="Enum">
  The current status of the evaluation task.
  Possible values:
  - **Pending**: The task has been created but not yet processed.
  - **Success**: The task was processed successfully.
  - **Failed**: The task encountered an error during processing.
</ResponseField>

<ResponseField name="Score" type="Number">
  The score assigned to the output by the metric type's evaluation criteria. **Example**: 0.85
</ResponseField>

<ResponseField name="Reason" type="Text">
  The explanation of the score assigned to the output by the [metric type's](/concepts/metric-type) evaluation criteria.
</ResponseField>

<ResponseField name="Error" type="Text">
  The error message if the task failed during processing.
</ResponseField>