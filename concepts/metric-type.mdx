---
title: "Metric Type"
description: "Ways to evaluate and score product performance"
icon: "function"
iconType: "solid"
---

import EvaluationCard from '/snippets/cards/evaluation-card.mdx';
import EvaluationTaskCard from '/snippets/cards/evaluation-task-card.mdx';
import SDKMetricsServiceCard from '/snippets/cards/sdk-metrics-service-card.mdx';

## What are Metric Types?

Metric types in Galtea define the specific criteria and methods used to evaluate the performance of your [product](/concepts/product). They determine how outputs are scored during [evaluation tasks](/concepts/product/evaluation/task), ensuring consistent and meaningful assessment.

<Info>
  Metric types are organization-wide and can be reused across multiple products.
</Info>

You can view and manage your metric types on the [Galtea dashboard](https://platform.galtea.ai/metrics).

## Metric Type Properties

When creating a metric type in Galtea, you'll need to provide the following information:

<ResponseField name="Name" type="Text" required>
  The name of the metric type. **Example**: "Factual Accuracy"
</ResponseField>

<ResponseField name="Evaluation Parameters" type="Text List" required>
  A list of strings indicating which parameters are relevant for this metric's evaluation criteria or steps. These parameters should be explicitly mentioned in your evaluation criteria or steps to ensure they're taken into account during assessment. The SDK expects these as strings (e.g., 'actual output').
  <table>
    <thead className='text-left'>
      <tr>
        <th>Parameter</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>**input**</td>
        <td>The prompt or query sent to the model. (Always required in the list).</td>
      </tr>
      <tr>
        <td>**actual output**</td>
        <td>The actual output generated by the model.</td>
      </tr>
      <tr>
        <td>**expected output**</td>
        <td>The ideal answer for the given input.</td>
      </tr>
      <tr>
        <td>**context**</td>
        <td>Additional background information provided to the model alongside the input.</td>
      </tr>
      <tr>
        <td>**retrieval context**</td>
        <td>The context retrieved by your RAG system before sending the user query to your LLM.</td>
      </tr>
    </tbody>
  </table>

  <Note>
    `input` (lowercase) must always be included in this list. At least one other parameter is required.
  </Note>

  <Info>
    You can directly reference these parameters in your criteria or evaluation steps. For example: "Evaluate if the **Actual Output** contains factually correct information that aligns with verified sources in the **Retrieval Context**."
  </Info>

  <Warning>
    To ensure accurate evaluation results, include only those parameters in your `evaluation_params` list that you've explicitly referenced in your criteria or evaluation steps.
    While you might refer to parameters like 'actual output' with synonyms (e.g., 'response') in your free-text criteria or steps, the `evaluation_params` list must use the exact predefined string values (e.g., 'actual output').
  </Warning>
</ResponseField>

<ResponseField name="Source" type="Text">
  Indicates the origin or provider of the metric type (e.g., 'geval', 'deepeval'). For custom-defined metrics where Galtea evaluates, this defaults to 'geval'. If scores are provided directly, this field might be less relevant or not set.
</ResponseField>

<ResponseField name="Definition" type="Text">
  A more technical definition or identifier for the metric, especially for Galtea's default metrics that map to specific underlying evaluators (e.g., a "toxicity" metric might have a definition like "toxicity_v2.0"). For custom metrics, this field might not be applicable.
</ResponseField>

<ResponseField name="Criteria" type="Text" required="conditional">
  High-level standards that define what aspects of a response matter for evaluation. **Example**: "Evaluate if the response contains factually correct information that aligns with verified sources. Penalize statements that contradict established knowledge or introduce speculation without
  citation."
</ResponseField>

<ResponseField name="Evaluation Steps" type="Text List" required="conditional">
  A structured set of checks that determine how a metric assesses correctness. **Example**:
  <ol>
    <li>Check if the 'actual output' contains facts that align with verified sources</li>
    <li>Identify any contradictions between the 'actual output' and established knowledge</li>
    <li>Penalize statements that introduce speculation without citing a credible source</li>
  </ol>
  <Note>You need to provide **either** *Criteria* **or** *Evaluation Steps*, but not both. Your choice depends on your preferred evaluation approach.</Note>
</ResponseField>

## Evaluation Criteria vs. Evaluation Steps

Understanding the difference between these two approaches is essential for creating effective metrics:

<CardGroup cols={2}>
  <Card title="Evaluation Criteria" icon="bullseye">
    **What matters** in a response, defining the high-level qualities or standards
  </Card>
  <Card title="Evaluation Steps" icon="list-check">
    **How to measure** a response's quality, providing specific assessment actions
  </Card>
</CardGroup>

### Evaluation Criteria

Evaluation criteria are high-level qualities or standards that define what makes a response good or bad. They outline fundamental aspects that should be assessed without specifying exactly how to measure them.

<AccordionGroup>
  <Accordion title="Purpose">
    <ul>
      <li>Ensure evaluations align with user needs and expectations</li>
      <li>Establish a framework for selecting appropriate metrics</li>
      <li>Guide AI model development by focusing on specific areas for improvement</li>
    </ul>
  </Accordion>
  <Accordion title="Examples">
    <ul>
      <li>
        <strong>Accuracy:</strong> The response must be factually correct and based on verified knowledge from the "context"
      </li>
      <li>
        <strong>Professionalism:</strong> The language should be respectful, clear, and aligned with professional communication standards
      </li>
      <li>
        <strong>Relevance:</strong> The response must directly address the user's "input" without unnecessary information
      </li>
      <li>
        <strong>Conciseness:</strong> The summary should be brief but still capture essential information
      </li>
      <li>
        <strong>Completeness:</strong> All key details from the original document ("context") must be included
      </li>
      <li>
        <strong>Fluency:</strong> The summary should be grammatically correct and easy to read
      </li>
    </ul>
  </Accordion>
</AccordionGroup>

<Info>Evaluation criteria define **what** matters in a response, serving as the foundation for meaningful assessment.</Info>

### Evaluation Steps

Evaluation steps are the specific actions taken to measure how well a response meets the evaluation criteria. These steps break down the assessment into concrete, structured processes that reference evaluation parameters.

<AccordionGroup>
  <Accordion title="Purpose">
    <ul>
      <li>Provide a systematic method for evaluating responses</li>
      <li>Reduce subjectivity by defining clear measurement techniques</li>
      <li>Allow customization based on the priorities of an AI system</li>
      <li>Explicitly reference evaluation parameters to ensure consistent assessment</li>
    </ul>
  </Accordion>
  <Accordion title="Example: Accuracy Steps">
    <ol>
      <li>Check if the **Actual Output** contains facts that align with verified sources provided in the **Retrieval Context**</li>
      <li>Identify any contradictions between the **Actual Output** and established knowledge in the **Context**</li>
      <li>Compare the **Actual Output** against the **Expected Output** for factual consistency</li>
      <li>Penalize statements in the **Actual Output** that introduce speculation without citing a credible source</li>
    </ol>
  </Accordion>
  <Accordion title="Example: Completeness Steps">
    <ol>
      <li>Compare the key points of the original document in **Context** with the **Actual Output**</li>
      <li>Identify any missing crucial information from the **Context** that would affect the meaning</li>
      <li>Compare the **Actual Output** against the **Expected Output** for coverage of essential points</li>
      <li>Score the **Actual Output** based on how many critical details from the **Input** and **Context** are retained</li>
    </ol>
  </Accordion>
</AccordionGroup>

<Info>Evaluation steps define **how** to measure a response's quality based on the evaluation criteria, making explicit reference to specific evaluation parameters like **Input**, **Actual Output**, **Expected Output**, **Retrieval Context**, and **Context**.</Info>

## Comparing Evaluation Approaches

The following table highlights the key differences between evaluation criteria and evaluation steps:

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Evaluation Criteria</th>
      <th>Evaluation Steps</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**Definition**</td>
      <td>High-level qualities that define what makes a response good or bad</td>
      <td>Step-by-step actions to measure a response's quality</td>
    </tr>
    <tr>
      <td>**Purpose**</td>
      <td>Establish broad goals for evaluation</td>
      <td>Provide a systematic method to assess responses</td>
    </tr>
    <tr>
      <td>**Focus**</td>
      <td>What should be measured</td>
      <td>How to measure it</td>
    </tr>
    <tr>
      <td>**Examples**</td>
      <td>Accuracy, conciseness, relevance, fluency</td>
      <td>Compare facts, check for contradictions, assess completeness</td>
    </tr>
    <tr>
      <td>**Flexibility**</td>
      <td>General principles that apply across many use cases</td>
      <td>Specific steps that vary depending on the system</td>
    </tr>
  </tbody>
</table>

## Custom Metrics

You have the flexibility to create custom metrics tailored to your specific use cases and evaluation preferences.
This feature enables you to define a metric with a name, conduct evaluations locally, and directly assign scores.

<Info>
  The platform cannot automatically evaluate custom metrics, as it lacks the necessary information. Therefore, you are responsible for uploading the evaluation results to visualize charts and data based on these metrics.
</Info>

## Related Concepts

<CardGroup cols={2}>
  <EvaluationCard />
  <EvaluationTaskCard />
</CardGroup>

## SDK Integration

<SDKMetricsServiceCard />
