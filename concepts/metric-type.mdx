---
title: "Metric Type"
description: "Ways to evaluate and score product performance"
icon: "function"
iconType: "solid"
---

## What are Metric Types?

Metric types in Galtea define the specific criteria and methods used to evaluate the performance of your [product](/concepts/product). They determine how outputs are scored during [evaluation tasks](/concepts/product/evaluation/task), ensuring consistent and meaningful assessment.

You can view and manage your metric types on the [Galtea dashboard](https://platform.galtea.ai/metrics).

## Metric Type Properties

When creating a metric type in Galtea, you'll need to provide the following information:

<ResponseField name="Name" type="Text" required>
  The name of the metric type. **Example**: "Factual Accuracy"
</ResponseField>

<Tabs>
  <Tab title="Criteria">
    <ResponseField name="Criteria" type="Text Area" required="conditional">
      High-level standards that define what aspects of a response matter for evaluation. **Example**: "Evaluate if the response contains factually correct information that aligns with verified sources. Penalize statements that contradict established knowledge or introduce speculation without
      citation."
    </ResponseField>
  </Tab>
  <Tab title="Evaluation Steps">
    <ResponseField name="Evaluation Steps" type="Text List" required="conditional">
      A structured set of checks that determine how a metric assesses correctness. **Example**:
      <ol>
        <li>Check if the response contains facts that align with verified sources</li>
        <li>Identify any contradictions between the response and established knowledge</li>
        <li>Penalize statements that introduce speculation without citing a credible source</li>
      </ol>
    </ResponseField>
  </Tab>
</Tabs>

<Note>The "Criteria" and "Evaluation Steps" fields are mutually exclusive. When creating a metric type, you'll be asked to choose between these validation methods, and only the selected field will be required.</Note>

## Evaluation Criteria vs. Evaluation Steps

Understanding the difference between these two approaches is essential for creating effective metrics:

<CardGroup cols={2}>
  <Card title="Evaluation Criteria" icon="bullseye">
    **What matters** in a response, defining the high-level qualities or standards
  </Card>
  <Card title="Evaluation Steps" icon="list-check">
    **How to measure** a response's quality, providing specific assessment actions
  </Card>
</CardGroup>

### Evaluation Criteria

Evaluation criteria are high-level qualities or standards that define what makes a response good or bad. They outline fundamental aspects that should be assessed without specifying exactly how to measure them.

<AccordionGroup>
  <Accordion title="Purpose">
    <ul>
      <li>Ensure evaluations align with user needs and expectations</li>
      <li>Establish a framework for selecting appropriate metrics</li>
      <li>Guide AI model development by focusing on specific areas for improvement</li>
    </ul>
  </Accordion>
  <Accordion title="Examples">
    <ul>
      <li>
        <strong>Accuracy:</strong> The response must be factually correct and based on verified knowledge
      </li>
      <li>
        <strong>Professionalism:</strong> The language should be respectful, clear, and aligned with professional communication standards
      </li>
      <li>
        <strong>Relevance:</strong> The response must directly address the user's question without unnecessary information
      </li>
      <li>
        <strong>Conciseness:</strong> The summary should be brief but still capture essential information
      </li>
      <li>
        <strong>Completeness:</strong> All key details from the original document must be included
      </li>
      <li>
        <strong>Fluency:</strong> The summary should be grammatically correct and easy to read
      </li>
    </ul>
  </Accordion>
</AccordionGroup>

<Info>Evaluation criteria define **what** matters in a response, serving as the foundation for meaningful assessment.</Info>

### Evaluation Steps

Evaluation steps are the specific actions taken to measure how well a response meets the evaluation criteria. These steps break down the assessment into concrete, structured processes.

<AccordionGroup>
  <Accordion title="Purpose">
    <ul>
      <li>Provide a systematic method for evaluating responses</li>
      <li>Reduce subjectivity by defining clear measurement techniques</li>
      <li>Allow customization based on the priorities of an AI system</li>
    </ul>
  </Accordion>
  <Accordion title="Example: Accuracy Steps">
    <ol>
      <li>Check if the response contains facts that align with verified sources (e.g., textbooks, official guidelines)</li>
      <li>Identify any contradictions between the chatbot's response and established knowledge</li>
      <li>Penalize statements that introduce speculation without citing a credible source</li>
    </ol>
  </Accordion>
  <Accordion title="Example: Completeness Steps">
    <ol>
      <li>Compare the key points of the original document with the generated summary</li>
      <li>Identify any missing crucial information that would affect the meaning</li>
      <li>Score the summary based on how many critical details are retained from the original text</li>
    </ol>
  </Accordion>
</AccordionGroup>

<Info>Evaluation steps define **how** to measure a response's quality based on the evaluation criteria.</Info>

## Comparing Evaluation Approaches

The following table highlights the key differences between evaluation criteria and evaluation steps:

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Evaluation Criteria</th>
      <th>Evaluation Steps</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**Definition**</td>
      <td>High-level qualities that define what makes a response good or bad</td>
      <td>Step-by-step actions to measure a response's quality</td>
    </tr>
    <tr>
      <td>**Purpose**</td>
      <td>Establish broad goals for evaluation</td>
      <td>Provide a systematic method to assess responses</td>
    </tr>
    <tr>
      <td>**Focus**</td>
      <td>What should be measured</td>
      <td>How to measure it</td>
    </tr>
    <tr>
      <td>**Examples**</td>
      <td>Accuracy, conciseness, relevance, fluency</td>
      <td>Compare facts, check for contradictions, assess completeness</td>
    </tr>
    <tr>
      <td>**Flexibility**</td>
      <td>General principles that apply across many use cases</td>
      <td>Specific steps that vary depending on the system</td>
    </tr>
  </tbody>
</table>

## Using Metric Types in Evaluations

Metric types play a crucial role in [evaluation tasks](/concepts/product/evaluation/task). When you create an evaluation task, you specify which metric type(s) to use for assessing the product's output.

```python
from galtea import Galtea

galtea = Galtea(api_key=API_KEY)

# Create an evaluation task using a specific metric type
result = galtea.evaluations.create_task(
    evaluation_id="eval_123",
    metric_type="factual_accuracy",
    input="What is the capital of France?",
    actual_output="The capital of France is Paris.",
    expected_output="Paris is the capital of France."
)
```

## Predefined Metric Types

Galtea provides several predefined metric types that you can use out of the box:

<CardGroup cols={2}>
  <Card title="Factual Accuracy" icon="check-double">
    Evaluates whether the output contains factually correct information
  </Card>
  <Card title="Relevance" icon="bullseye">
    Assesses how well the output addresses the input question or task
  </Card>
  <Card title="Clarity" icon="glasses">
    Measures how clear, concise, and understandable the output is
  </Card>
  <Card title="Safety" icon="shield-halved">
    Evaluates whether the output avoids harmful, offensive, or inappropriate content
  </Card>
</CardGroup>

## Custom Metric Types

You can also create custom metric types tailored to your specific evaluation needs:

<Steps>
  <Step title="Access the Metrics Dashboard">Navigate to the [Metrics section](https://platform.galtea.ai/metrics) in the Galtea dashboard</Step>
  <Step title="Create a New Metric Type">Click the "New Metric Type" button and enter the required information</Step>
  <Step title="Define Validation Method">Choose between "Criteria" or "Evaluation Steps" based on your preference</Step>
  <Step title="Save and Test">Save your new metric type and test it in an evaluation task</Step>
</Steps>

## Related Concepts

<CardGroup cols={2}>
  <Card title="Evaluation" icon="magnifying-glass-chart" href="/concepts/product/evaluation">
    The link between a product version and a test
  </Card>
  <Card title="Evaluation Task" icon="clipboard-check" href="/concepts/product/evaluation/task">
    The assessment of a test challenge using a metric type
  </Card>
</CardGroup>
