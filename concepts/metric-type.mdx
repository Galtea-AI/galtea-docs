---
title: "Metric Type"
description: "Ways to evaluate and score product performance"
icon: "function"
iconType: "solid"
---

import EvaluationCard from '/snippets/cards/evaluation-card.mdx';
import EvaluationTaskCard from '/snippets/cards/evaluation-task-card.mdx';
import SDKMetricsServiceCard from '/snippets/cards/sdk-metrics-service-card.mdx';

## What are Metric Types?

Metric types in Galtea define the specific criteria and methods used to evaluate the performance of your [product](/concepts/product). They determine how outputs are scored during [evaluation tasks](/concepts/product/version/session/evaluation/task), ensuring consistent and meaningful assessment.

<Info>
  Metric types are organization-wide and can be reused across multiple products.
</Info>

You can **create**, view and manage your metric types on the [Galtea dashboard](https://platform.galtea.ai/) or programmatically using the [Galtea SDK](/sdk/api/metrics/service).

## Conceptual categories

At Galtea, we use two types of metrics to evaluate large language model (LLM) outputs: **deterministic** and **non-deterministic**.

- **Deterministic metrics** are rule-based and computed using strict logic, such as SQL queries or structural checks. These include things like answer format validation, presence of required fields, or exact string matches. Their results are consistent and reproducible. See more in [our example](/sdk/tutorials/create-evaluation-with-custom-scores).

  Common examples of deterministic checks include:
  *   Answer format validation (e.g., ensuring the output is a valid JSON or follows a specific template)
  *   Presence of required fields (e.g., checking if all necessary information is included in the response)
  *   Exact string matches (e.g., verifying if a specific keyword or phrase is present)
  *   Numerical range checks (e.g., confirming a value falls within an acceptable range)
  *   Boolean condition checks (e.g., ensuring a specific condition evaluates to true or false as expected)

  <Info>
    The platform cannot automatically evaluate *deterministic metrics*, as it lacks the necessary information. Therefore, you are responsible for uploading the evaluation results to visualize charts and data based on these metrics.
  </Info>

- **Non-deterministic metrics** are powered by the **G-Eval** evaluation framework, an LLM-as-a-judge methodology, originally introduced in the paper [“NLG Evaluation using GPT-4 with Better Human Alignment”](https://arxiv.org/abs/2303.16634). G-Eval follows a structured two-step process (chain-of-thought generation and score computation) to rate outputs. These metrics assess aspects like factuality, coherence, helpfulness, or tone.

<Info title="Applicability to Different Output Types">
  Galtea metrics are versatile and can be applied to **any type of output**, including strings, numbers, and boolean values.

  *   **For deterministic metrics:** Evaluating numerical or boolean outputs is straightforward. For example, you can check if a returned numerical value is within a valid range or if a boolean output matches a specific condition.
  *   **For non-deterministic metrics:** While typically used for open-ended text, they can also assess the reasoning or justification behind a numerical or boolean value when needed. For instance, verifying whether a model's numeric prediction aligns with the provided context or input data.
</Info>

## List of metrics available in the Galtea Platform

<Note>
  The following table provides a summary of the default metrics available in the Galtea platform. You can also create custom metrics tailored to your specific needs.
</Note>

| Metric | Category | Description |
| --- | --- | --- |
| **[Factual Accuracy](/concepts/metric-type/factual-accuracy)** | RAG | Measures the quality of your RAG pipeline's generator by evaluating whether the actual_output factually aligns with the expected_output. |
| **[Answer Relevancy](/concepts/metric-type/answer-relevancy)** | RAG | Measures the quality of your RAG pipeline's generator by evaluating how relevant the actual_output of your LLM application is compared to the provided input. |
| **[Faithfulness](/concepts/metric-type/faithfulness)** | RAG | Measures the quality of your RAG pipeline's generator by evaluating whether the actual_output factually aligns with the contents of your retrieval_context. |
| **[Contextual Precision](/concepts/metric-type/contextual-precision)** | RAG | Measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval_context that are relevant to the given input are ranked higher than irrelevant ones. |
| **[Contextual Recall](/concepts/metric-type/contextual-recall)** | RAG | Measures the quality of your RAG pipeline's retriever by evaluating the extent of which the retrieval_context aligns with the expected_output. |
| **[Contextual Relevancy](/concepts/metric-type/contextual-relevancy)** | RAG | Measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval_context for a given input. |
| **[Role Adherence](/concepts/metric-type/role-adherence)** | Conversational | Determines whether your LLM chatbot is able to adhere to its given role throughout a conversation. |
| **[Conversation Completeness](/concepts/metric-type/conversation-completeness)** | Conversational | Determines whether your LLM chatbot is able to complete an end-to-end conversation by satisfying user needs throughout a conversation. |
| **[Conversation Relevancy](/concepts/metric-type/conversation-relevancy)** | Conversational | Determines whether your LLM chatbot is able to consistently generate relevant responses throughout a conversation. |
| **[Knowledge Retention](/concepts/metric-type/knowledge-retention)** | Conversational | Assesses the ability of your LLM chatbot to retain factual information presented throughout a conversation. |
| **[Non-Toxic](/concepts/metric-type/non-toxic)** | Red Teaming | Determines whether the responses of your LLM based product responds are free of toxic language. |
| **[Unbiased](/concepts/metric-type/unbiased)** | Red Teaming | Determine whether your LLM output is free of gender, racial, or political bias. |


## Metric Type Properties

<ResponseField name="Name" type="Text" required>
  The name of the metric type. **Example**: "Factual Accuracy"
</ResponseField>

<ResponseField name="Description" type="Text" optional>
  A brief description of what the metric type evaluates.
</ResponseField>

<ResponseField name="Documentation URL" type="URL" optional>
  A URL pointing to more detailed documentation about the metric type.
</ResponseField>

<ResponseField name="Tags" type="Text List" optional>
  The tags for the metric type so it can be easily identified and categorized. **Example**: ["RAG", "Conversational"]
</ResponseField>

<ResponseField name="Validation method" type="Enum" required>
  The method used to evaluate the metric. This can be either:

  <Tabs>
  <Tab title="None">
    No validation method is applied. This means the metric type is *not* automatically evaluated by Galtea and is intended for *deterministic metrics*.
  </Tab>
  <Tab title="Criteria">
    Used for *non-deterministic metrics*.
    <ResponseField name="Criteria" type="Text" required="conditional">
      High-level standards that define what aspects of a response matter for evaluation. **Example**: "Evaluate if the response contains factually correct information that aligns with verified sources. Penalize statements that contradict established knowledge or introduce speculation without
      citation."
    </ResponseField>
  </Tab>
  <Tab title="Evaluation Steps">
    Used for *non-deterministic metrics*.
    <ResponseField name="Evaluation Steps" type="Text List" required="conditional">
      A structured set of checks that determine how a metric assesses correctness. **Example**:
      <ol>
        <li>Check if the 'actual output' contains facts that align with verified sources</li>
        <li>Identify any contradictions between the 'actual output' and established knowledge</li>
        <li>Penalize statements that introduce speculation without citing a credible source</li>
      </ol>
      <Note>You need to provide **either** *Criteria* **or** *Evaluation Steps*, but not both. Your choice depends on your preferred evaluation approach.</Note>
    </ResponseField>
  </Tab>
</Tabs>
</ResponseField>

<ResponseField name="Evaluation Parameters" type="List[Enum]" required>
  <Info>
    This is not required if the validation method is set to "None".
  </Info>
  A list of relevant parameters for this metric's evaluation criteria or steps. These parameters should be explicitly mentioned in your evaluation criteria or steps to ensure they're taken into account during assessment.
  <table>
    <thead className='text-left'>
      <tr>
        <th>Parameter</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>**input**</td>
        <td>The prompt or query sent to the model. (Always required in the list).</td>
      </tr>
      <tr>
        <td>**actual output**</td>
        <td>The actual output generated by the model.</td>
      </tr>
      <tr>
        <td>**expected output**</td>
        <td>The ideal answer for the given input.</td>
      </tr>
      <tr>
        <td>**context**</td>
        <td>Additional background information provided to the model alongside the input.</td>
      </tr>
      <tr>
        <td>**retrieval context**</td>
        <td>The context retrieved by your RAG system before sending the user query to your LLM.</td>
      </tr>
    </tbody>
  </table>

  <Note>
    `input` (lowercase) must always be included in this list.
  </Note>

  <Info>
    You can directly reference these parameters in your criteria or evaluation steps. For example: "Evaluate if the *Actual Output* contains factually correct information that aligns with verified sources in the *Retrieval Context*."
  </Info>

  <Warning>
    To ensure accurate evaluation results, include only parameters that you've explicitly referenced in your criteria or evaluation steps.
  </Warning>
</ResponseField>

## G-Eval

<Note title="Our Evaluation Model">
  The non-deterministic metrics, powered by LLM-as-a-judge (G-Eval), utilize models that have demonstrated the best performance in our internal benchmarks and testing. We are committed to continuously evolving and improving these evaluator models to ensure the highest quality assessments over time.
</Note>

### Evaluation Criteria vs. Evaluation Steps

When using non-deterministic metrics, you can choose between two approaches: **evaluation criteria** and **evaluation steps**. Both methods are designed to assess the quality of LLM outputs, but they differ in their focus and structure.

<CardGroup cols={2}>
  <Card title="Evaluation Criteria" icon="bullseye">
    **What matters** in a response, defining the high-level qualities or standards
  </Card>
  <Card title="Evaluation Steps" icon="list-check">
    **How to measure** a response's quality, providing specific assessment actions
  </Card>
</CardGroup>

#### Evaluation Criteria

Evaluation criteria are high-level qualities or standards that define what makes a response good or bad. They outline fundamental aspects that should be assessed without specifying exactly how to measure them.

<AccordionGroup>
  <Accordion title="Purpose">
    <ul>
      <li>Ensure evaluations align with user needs and expectations</li>
      <li>Establish a framework for selecting appropriate metrics</li>
      <li>Guide AI model development by focusing on specific areas for improvement</li>
    </ul>
  </Accordion>
  <Accordion title="Examples">
    <ul>
      <li>
        <strong>Accuracy:</strong> The response must be factually correct and based on verified knowledge from the "context"
      </li>
      <li>
        <strong>Professionalism:</strong> The language should be respectful, clear, and aligned with professional communication standards
      </li>
      <li>
        <strong>Relevance:</strong> The response must directly address the user's "input" without unnecessary information
      </li>
      <li>
        <strong>Conciseness:</strong> The summary should be brief but still capture essential information
      </li>
      <li>
        <strong>Completeness:</strong> All key details from the original document ("context") must be included
      </li>
      <li>
        <strong>Fluency:</strong> The summary should be grammatically correct and easy to read
      </li>
    </ul>
  </Accordion>
</AccordionGroup>

<Info>Evaluation criteria define **what** matters in a response, serving as the foundation for meaningful assessment.</Info>

#### Evaluation Steps

Evaluation steps are the specific actions taken to measure how well a response meets the evaluation criteria. These steps break down the assessment into concrete, structured processes that reference evaluation parameters.

<AccordionGroup>
  <Accordion title="Purpose">
    <ul>
      <li>Provide a systematic method for evaluating responses</li>
      <li>Reduce subjectivity by defining clear measurement techniques</li>
      <li>Allow customization based on the priorities of an AI system</li>
      <li>Explicitly reference evaluation parameters to ensure consistent assessment</li>
    </ul>
  </Accordion>
  <Accordion title="Example: Accuracy Steps">
    <ol>
      <li>Check if the **Actual Output** contains facts that align with verified sources provided in the **Retrieval Context**</li>
      <li>Identify any contradictions between the **Actual Output** and established knowledge in the **Context**</li>
      <li>Compare the **Actual Output** against the **Expected Output** for factual consistency</li>
      <li>Penalize statements in the **Actual Output** that introduce speculation without citing a credible source</li>
    </ol>
  </Accordion>
  <Accordion title="Example: Completeness Steps">
    <ol>
      <li>Compare the key points of the original document in **Context** with the **Actual Output**</li>
      <li>Identify any missing crucial information from the **Context** that would affect the meaning</li>
      <li>Compare the **Actual Output** against the **Expected Output** for coverage of essential points</li>
      <li>Score the **Actual Output** based on how many critical details from the **Input** and **Context** are retained</li>
    </ol>
  </Accordion>
</AccordionGroup>

<Info>Evaluation steps define **how** to measure a response's quality based on the evaluation criteria, making explicit reference to specific evaluation parameters like **Input**, **Actual Output**, **Expected Output**, **Retrieval Context**, and **Context**.</Info>

<Note>When you *do* supply explicit `evaluation_steps`, G-Eval skips auto-generation—yielding more reliable, reproducible scores ideal for production.</Note>

#### Comparing Evaluation Approaches

The following table highlights the key differences between evaluation criteria and evaluation steps:

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Evaluation Criteria</th>
      <th>Evaluation Steps</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**Definition**</td>
      <td>High-level qualities that define what makes a response good or bad</td>
      <td>Step-by-step actions to measure a response's quality</td>
    </tr>
    <tr>
      <td>**Purpose**</td>
      <td>Establish broad goals for evaluation</td>
      <td>Provide a systematic method to assess responses</td>
    </tr>
    <tr>
      <td>**Focus**</td>
      <td>What should be measured</td>
      <td>How to measure it</td>
    </tr>
    <tr>
      <td>**Examples**</td>
      <td>Accuracy, conciseness, relevance, fluency</td>
      <td>Compare facts, check for contradictions, assess completeness</td>
    </tr>
    <tr>
      <td>**Flexibility**</td>
      <td>General principles that apply across many use cases</td>
      <td>Specific steps that vary depending on the system</td>
    </tr>
  </tbody>
</table>

### Evaluation Details

**Two-step process**  
1. Chain-of-Thought Generation (optional): If you do not provide `evaluation_steps`, G-Eval auto-generates them from your `criteria` via a chain-of-thought prompt—making setup fast but introducing some variability.  
2. Score Computation: G-Eval prompts the LLM with the steps and test-case parameters (`input`, `actual_output`, etc.) to rate on a **1–5** scale, then normalizes the result to **0–1** via token-probability weighting.

## SDK Integration

The Galtea SDK allows you to create, view, and manage metric types programmatically.

<SDKMetricsServiceCard />

## Related Concepts

<CardGroup cols={2}>
  <EvaluationCard />
  <EvaluationTaskCard />
</CardGroup>
